{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0m1nbfGay6R"
   },
   "source": [
    "## Voxel VLMap Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "root_dir = !pwd\n",
    "root_dir = root_dir[0]\n",
    "data_dir = os.path.join(root_dir, \"data\", \"5LpN3gDmAk7_1\")\n",
    "\n",
    "colors_path = root_dir + \"/maps/colormap.npy\"\n",
    "features_path = root_dir + \"/maps/featuremap.npy\"\n",
    "coordinates_path = root_dir + \"/maps/coordinates.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bx_EfAUoay6S"
   },
   "outputs": [],
   "source": [
    "# setup parameters\n",
    "# @markdown meters per cell size\n",
    "cs = 0.05 # @param {type: \"number\"}\n",
    "voxel_size = 0.05 # @param {type: \"number\"}\n",
    "# @markdown map resolution (gs x gs)\n",
    "gs = 1000 # @param {type: \"integer\"}\n",
    "# @markdown camera height (used for filtering out points on the floor)\n",
    "camera_height = 1.5 # @param {type: \"number\"}\n",
    "# @markdown depth pixels subsample rate\n",
    "depth_sample_rate = 100 # @param {type: \"integer\"}\n",
    "# @markdown data where rgb, depth, pose are loaded and map are saved\n",
    "data_dir = data_dir # @param {type: \"string\"}\n",
    "\n",
    "img_save_dir = data_dir\n",
    "mask_version = 1\n",
    "crop_size = 480 # 480\n",
    "base_size = 520 # 520"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import open3d\n",
    "import clip\n",
    "# import torch\n",
    "lang = \"door,chair,ground,ceiling,other\"\n",
    "labels = lang.split(\",\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_version = \"ViT-B/32\"\n",
    "clip_feat_dim = {'RN50': 1024, 'RN101': 512, 'RN50x4': 640, 'RN50x16': 768,\n",
    "                'RN50x64': 1024, 'ViT-B/32': 512, 'ViT-B/16': 512, 'ViT-L/14': 768}[clip_version]\n",
    "\n",
    "print(\"Loading CLIP model...\")\n",
    "clip_model, preprocess = clip.load(clip_version, device=device)  # clip.available_models()\n",
    "\n",
    "clip_model.to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__file__:  /Users/anthony/Documents/UW/Classes/SP24/CSE_571_Robotics/Project/vlmaps/examples/context.py\n",
      "imported path: /Users/anthony/Documents/UW/Classes/SP24/CSE_571_Robotics/Project/vlmaps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/test/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "# import open3d\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.clip_mapping_utils import load_pose, load_semantic, load_obj2cls_dict, save_map, cvt_obj_id_2_cls_id, depth2pc, transform_pc, get_sim_cam_mat, pos2grid_id, project_point\n",
    "from lseg.modules.models.lseg_net import LSegEncNet\n",
    "from lseg.additional_utils.models import resize_image, pad_image, crop_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Checkpoints...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from lseg.modules.models.lseg_net import LSegEncNet\n",
    "\n",
    "lang_token = clip.tokenize(labels)\n",
    "lang_token = lang_token.to(device)\n",
    "with torch.no_grad():\n",
    "    text_feats = clip_model.encode_text(lang_token)\n",
    "    text_feats = text_feats / text_feats.norm(dim=-1, keepdim=True)\n",
    "text_feats = text_feats.cpu().numpy()\n",
    "model = LSegEncNet(lang, arch_option=0,\n",
    "                    block_depth=0,\n",
    "                    activation='lrelu',\n",
    "                    crop_size=crop_size)\n",
    "model_state_dict = model.state_dict()\n",
    "print(\"Loading Checkpoints...\")\n",
    "if device == \"cpu\":\n",
    "    pretrained_state_dict = torch.load(\"lseg/checkpoints/demo_e200.ckpt\", map_location=torch.device('cpu'))\n",
    "else:\n",
    "    pretrained_state_dict = torch.load(\"lseg/checkpoints/demo_e200.ckpt\")\n",
    "pretrained_state_dict = {k.lstrip('net.'): v for k, v in pretrained_state_dict['state_dict'].items()}\n",
    "model_state_dict.update(pretrained_state_dict)\n",
    "model.load_state_dict(pretrained_state_dict)\n",
    "\n",
    "model.eval()\n",
    "if device == \"cuda\":\n",
    "    model = model.cuda()\n",
    "\n",
    "norm_mean= [0.5, 0.5, 0.5]\n",
    "norm_std = [0.5, 0.5, 0.5]\n",
    "padding = [0.0] * 3\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This thing outputs a image segmentation mask as a np array\n",
    "def get_lseg_feat(model: LSegEncNet, image: np.array, labels, transform, crop_size=480, \\\n",
    "                 base_size=520, norm_mean=[0.5, 0.5, 0.5], norm_std=[0.5, 0.5, 0.5]):\n",
    "    vis_image = image.copy()\n",
    "    if device == \"cpu\":\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    else:\n",
    "        image = transform(image).unsqueeze(0).cuda()\n",
    "    img = image[0].permute(1,2,0)\n",
    "    img = img * 0.5 + 0.5\n",
    "    \n",
    "    batch, _, h, w = image.size() # batch is dimension 1, ignoring channel, y, x\n",
    "    stride_rate = 2.0/3.0\n",
    "    stride = int(crop_size * stride_rate)\n",
    "\n",
    "    long_size = base_size\n",
    "    if h > w:\n",
    "        height = long_size\n",
    "        width = int(1.0 * w * long_size / h + 0.5)\n",
    "        short_size = width\n",
    "    else:\n",
    "        width = long_size\n",
    "        height = int(1.0 * h * long_size / w + 0.5)\n",
    "        short_size = height\n",
    "\n",
    "\n",
    "    cur_img = resize_image(image, height, width, **{'mode': 'bilinear', 'align_corners': True})\n",
    "\n",
    "    if long_size <= crop_size:\n",
    "        pad_img = pad_image(cur_img, norm_mean,\n",
    "                            norm_std, crop_size)\n",
    "        print(pad_img.shape)\n",
    "        with torch.no_grad():\n",
    "            outputs, logits = model(pad_img, labels)\n",
    "        outputs = crop_image(outputs, 0, height, 0, width)\n",
    "    else:\n",
    "        if short_size < crop_size:\n",
    "            # pad if needed\n",
    "            pad_img = pad_image(cur_img, norm_mean,\n",
    "                                norm_std, crop_size)\n",
    "        else:\n",
    "            pad_img = cur_img\n",
    "        _,_,ph,pw = pad_img.shape #.size()\n",
    "        assert(ph >= height and pw >= width)\n",
    "        h_grids = int(math.ceil(1.0 * (ph-crop_size)/stride)) + 1\n",
    "        w_grids = int(math.ceil(1.0 * (pw-crop_size)/stride)) + 1\n",
    "\n",
    "        if device == \"cpu\":\n",
    "            with torch.cuda.device_of(image):\n",
    "                with torch.no_grad():\n",
    "                    outputs = image.new().resize_(batch, model.out_c,ph,pw).zero_()\n",
    "                    logits_outputs = image.new().resize_(batch, len(labels),ph,pw).zero_()\n",
    "                count_norm = image.new().resize_(batch,1,ph,pw).zero_()\n",
    "        else:\n",
    "            with torch.cuda.device_of(image):\n",
    "                with torch.no_grad():\n",
    "                    outputs = image.new().resize_(batch, model.out_c,ph,pw).zero_().cuda()\n",
    "                    logits_outputs = image.new().resize_(batch, len(labels),ph,pw).zero_().cuda()\n",
    "                count_norm = image.new().resize_(batch,1,ph,pw).zero_().cuda()\n",
    "        # grid evaluation\n",
    "        for idh in range(h_grids):\n",
    "            for idw in range(w_grids):\n",
    "                h0 = idh * stride\n",
    "                w0 = idw * stride\n",
    "                h1 = min(h0 + crop_size, ph)\n",
    "                w1 = min(w0 + crop_size, pw)\n",
    "                crop_img = crop_image(pad_img, h0, h1, w0, w1)\n",
    "                # pad if needed\n",
    "                pad_crop_img = pad_image(crop_img, norm_mean,\n",
    "                                            norm_std, crop_size)\n",
    "                with torch.no_grad():\n",
    "                    output, logits = model(pad_crop_img, labels)\n",
    "                cropped = crop_image(output, 0, h1-h0, 0, w1-w0)\n",
    "                cropped_logits = crop_image(logits, 0, h1-h0, 0, w1-w0)\n",
    "                outputs[:,:,h0:h1,w0:w1] += cropped\n",
    "                logits_outputs[:,:,h0:h1,w0:w1] += cropped_logits\n",
    "                count_norm[:,:,h0:h1,w0:w1] += 1\n",
    "        assert((count_norm==0).sum()==0)\n",
    "        outputs = outputs / count_norm\n",
    "        logits_outputs = logits_outputs / count_norm\n",
    "        outputs = outputs[:,:,:height,:width]\n",
    "        logits_outputs = logits_outputs[:,:,:height,:width]\n",
    "    outputs = outputs.cpu()\n",
    "    outputs = outputs.numpy() # B, D, H, W\n",
    "    predicts = [torch.max(logit, 0)[1].cpu().numpy() for logit in logits_outputs]\n",
    "    pred = predicts[0]\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def load_depth(depth_filepath):\n",
    "    with open(depth_filepath, 'rb') as f:\n",
    "        depth = np.load(f)\n",
    "    return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading scene /Users/anthony/Documents/UW/Classes/SP24/CSE_571_Robotics/Project/vlmaps/data/5LpN3gDmAk7_1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1159"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"loading scene {img_save_dir}\")\n",
    "rgb_dir = os.path.join(img_save_dir, \"rgb\")\n",
    "depth_dir = os.path.join(img_save_dir, \"depth\")\n",
    "pose_dir = os.path.join(img_save_dir, \"pose\")\n",
    "semantic_dir = os.path.join(img_save_dir, \"semantic\")\n",
    "\n",
    "rgb_list = sorted(os.listdir(rgb_dir), key=lambda x: int(\n",
    "    x.split(\"_\")[-1].split(\".\")[0]))\n",
    "depth_list = sorted(os.listdir(depth_dir), key=lambda x: int(\n",
    "    x.split(\"_\")[-1].split(\".\")[0]))\n",
    "pose_list = sorted(os.listdir(pose_dir), key=lambda x: int(\n",
    "    x.split(\"_\")[-1].split(\".\")[0]))\n",
    "pose_list = sorted(os.listdir(pose_dir), key=lambda x: int(\n",
    "    x.split(\"_\")[-1].split(\".\")[0]))\n",
    "semantic_list = sorted(os.listdir(semantic_dir), key=lambda x: int(\n",
    "    x.split(\"_\")[-1].split(\".\")[0]))\n",
    "\n",
    "rgb_list = [os.path.join(rgb_dir, x) for x in rgb_list]\n",
    "depth_list = [os.path.join(depth_dir, x) for x in depth_list]\n",
    "pose_list = [os.path.join(pose_dir, x) for x in pose_list]\n",
    "semantic_list = [os.path.join(semantic_dir, x) for x in semantic_list]\n",
    "\n",
    "tf_list = []\n",
    "data_iter = zip(rgb_list, depth_list, semantic_list, pose_list)\n",
    "len(rgb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos, rot = load_pose(pose_list[1])\n",
    "rot_ro_cam = np.eye(3)\n",
    "rot_ro_cam[1, 1] = -1\n",
    "rot_ro_cam[2, 2] = -1\n",
    "rot = rot @ rot_ro_cam\n",
    "pos[1] += camera_height\n",
    "\n",
    "pose = np.eye(4)\n",
    "pose[:3, :3] = rot\n",
    "pose[:3, 3] = pos.reshape(-1)\n",
    "\n",
    "tf_list.append(pose)\n",
    "if len(tf_list) == 1:\n",
    "    init_tf_inv = np.linalg.inv(tf_list[0]) \n",
    "\n",
    "tf = init_tf_inv @ pose\n",
    "p = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = load_depth(depth_list[0])\n",
    "pc, mask = depth2pc(depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m pc \u001b[38;5;241m=\u001b[39m pc[:, mask]\n\u001b[1;32m     44\u001b[0m pc_global \u001b[38;5;241m=\u001b[39m transform_pc(pc, tf)\n\u001b[0;32m---> 46\u001b[0m pix_feats \u001b[38;5;241m=\u001b[39m \u001b[43mget_lseg_feat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m rgb_cam_mat \u001b[38;5;241m=\u001b[39m get_sim_cam_mat(rgb\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], rgb\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     49\u001b[0m feat_cam_mat \u001b[38;5;241m=\u001b[39m get_sim_cam_mat(pix_feats\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], pix_feats\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m])\n",
      "Cell \u001b[0;32mIn[6], line 72\u001b[0m, in \u001b[0;36mget_lseg_feat\u001b[0;34m(model, image, labels, transform, crop_size, base_size, norm_mean, norm_std)\u001b[0m\n\u001b[1;32m     69\u001b[0m pad_crop_img \u001b[38;5;241m=\u001b[39m pad_image(crop_img, norm_mean,\n\u001b[1;32m     70\u001b[0m                             norm_std, crop_size)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 72\u001b[0m     output, logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad_crop_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m cropped \u001b[38;5;241m=\u001b[39m crop_image(output, \u001b[38;5;241m0\u001b[39m, h1\u001b[38;5;241m-\u001b[39mh0, \u001b[38;5;241m0\u001b[39m, w1\u001b[38;5;241m-\u001b[39mw0)\n\u001b[1;32m     74\u001b[0m cropped_logits \u001b[38;5;241m=\u001b[39m crop_image(logits, \u001b[38;5;241m0\u001b[39m, h1\u001b[38;5;241m-\u001b[39mh0, \u001b[38;5;241m0\u001b[39m, w1\u001b[38;5;241m-\u001b[39mw0)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/UW/Classes/SP24/CSE_571_Robotics/Project/vlmaps/lseg/modules/models/lseg_net.py:300\u001b[0m, in \u001b[0;36mLSegEnc.forward\u001b[0;34m(self, x, labelset)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels_last \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     x\u001b[38;5;241m.\u001b[39mcontiguous(memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mchannels_last)\n\u001b[0;32m--> 300\u001b[0m layer_1, layer_2, layer_3, layer_4 \u001b[38;5;241m=\u001b[39m \u001b[43mforward_vit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m layer_1_rn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscratch\u001b[38;5;241m.\u001b[39mlayer1_rn(layer_1)\n\u001b[1;32m    303\u001b[0m layer_2_rn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscratch\u001b[38;5;241m.\u001b[39mlayer2_rn(layer_2)\n",
      "File \u001b[0;32m~/Documents/UW/Classes/SP24/CSE_571_Robotics/Project/vlmaps/lseg/modules/models/lseg_vit.py:103\u001b[0m, in \u001b[0;36mforward_vit\u001b[0;34m(pretrained, x)\u001b[0m\n\u001b[1;32m    100\u001b[0m b, c, h, w \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# encoder\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m glob \u001b[38;5;241m=\u001b[39m \u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_flex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m layer_1 \u001b[38;5;241m=\u001b[39m pretrained\u001b[38;5;241m.\u001b[39mactivations[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    106\u001b[0m layer_2 \u001b[38;5;241m=\u001b[39m pretrained\u001b[38;5;241m.\u001b[39mactivations[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/UW/Classes/SP24/CSE_571_Robotics/Project/vlmaps/lseg/modules/models/lseg_vit.py:186\u001b[0m, in \u001b[0;36mforward_flex\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    183\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_drop(x)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 186\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test/lib/python3.8/site-packages/timm/models/vision_transformer.py:268\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 268\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    269\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))))\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test/lib/python3.8/site-packages/timm/models/vision_transformer.py:217\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    216\u001b[0m     B, N, C \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 217\u001b[0m     qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqkv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(B, N, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, C \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m    218\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)   \u001b[38;5;66;03m# make torchscript happy (cannot use tensor as tuple)\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     attn \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/test/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils.voxel import Voxel, GetVoxelCoor\n",
    "\n",
    "# lang = \"door,chair,ground,ceiling,other\"\n",
    "lang = \"ground,other\"\n",
    "labels = lang.split(\",\")\n",
    "\n",
    "voxel_grid = {}\n",
    "\n",
    "for frame_index, data_sample in enumerate(data_iter): \n",
    "    # print(frame_index)   \n",
    "    rgb_path, depth_path, semantic_path, pose_path = data_sample\n",
    "    \n",
    "    bgr = cv2.imread(rgb_path)\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # read pose\n",
    "    pos, rot = load_pose(pose_path)  # z backward, y upward, x to the right\n",
    "    rot_ro_cam = np.eye(3)\n",
    "    rot_ro_cam[1, 1] = -1\n",
    "    rot_ro_cam[2, 2] = -1\n",
    "    rot = rot @ rot_ro_cam\n",
    "    pos[1] += camera_height\n",
    "\n",
    "    pose = np.eye(4)\n",
    "    pose[:3, :3] = rot\n",
    "    pose[:3, 3] = pos.reshape(-1)\n",
    "\n",
    "    tf_list.append(pose)\n",
    "    if len(tf_list) == 1:\n",
    "        init_tf_inv = np.linalg.inv(tf_list[0]) \n",
    "\n",
    "    tf = init_tf_inv @ pose\n",
    "\n",
    "    depth = load_depth(depth_path)\n",
    "    \n",
    "    # transform all points to the global frame\n",
    "    pc, mask = depth2pc(depth)\n",
    "    shuffle_mask = np.arange(pc.shape[1]) \n",
    "    np.random.shuffle(shuffle_mask)\n",
    "    shuffle_mask = shuffle_mask[::depth_sample_rate]\n",
    "    mask = mask[shuffle_mask]\n",
    "    pc = pc[:, shuffle_mask]\n",
    "    pc = pc[:, mask]\n",
    "    pc_global = transform_pc(pc, tf)\n",
    "\n",
    "    pix_feats = get_lseg_feat(model, rgb, labels, transform, crop_size, base_size, norm_mean, norm_std)\n",
    "\n",
    "    rgb_cam_mat = get_sim_cam_mat(rgb.shape[0], rgb.shape[1])\n",
    "    feat_cam_mat = get_sim_cam_mat(pix_feats.shape[2], pix_feats.shape[3])\n",
    "\n",
    "    for pixel_index, (p, p_local) in enumerate(zip(pc_global.T, pc.T)):\n",
    "\n",
    "        single_global_point = (tf @ np.vstack([p_local.reshape(3,1), np.ones((1, 1))]) )[:3]\n",
    "\n",
    "        rgb_px, rgb_py, rgb_pz = project_point(rgb_cam_mat, p_local)\n",
    "        rgb_v = rgb[rgb_py, rgb_px, :].reshape(1,3)\n",
    "\n",
    "        voxel_coor = GetVoxelCoor(single_global_point, voxel_size)\n",
    "        voxel_key = str(voxel_coor[0,0]) + \",\" + str(voxel_coor[1,0]) + \",\" + str(voxel_coor[2,0])\n",
    "        if voxel_key not in voxel_grid:\n",
    "            voxel_grid[voxel_key] = Voxel(voxel_coor, pix_feats.shape[1]) #if not in voxel grid, then make it\n",
    "        voxel_grid[voxel_key].update_color((rgb_v/ 255.0))\n",
    "         \n",
    "        px, py, pz = project_point(feat_cam_mat, p_local)\n",
    "        if not (px < 0 or py < 0 or px >= pix_feats.shape[3] or py >= pix_feats.shape[2]):\n",
    "            feat = pix_feats[0, :, py, px] #these are for finding the corresponding features for that pixel in the feature matrix\n",
    "            voxel_grid[voxel_key].update_feature(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/anthony/Documents/UW/Classes/SP24/CSE_571_Robotics/Project/vlmaps/maps/colormap.npy is saved.\n",
      "/Users/anthony/Documents/UW/Classes/SP24/CSE_571_Robotics/Project/vlmaps/maps/featuremap.npy is saved.\n",
      "/Users/anthony/Documents/UW/Classes/SP24/CSE_571_Robotics/Project/vlmaps/maps/coordinates.npy is saved.\n"
     ]
    }
   ],
   "source": [
    "save_map(colors_path, np.array([i.colors for i in voxel_grid.values()]))\n",
    "save_map(features_path, np.array([i.features for i in voxel_grid.values()]))\n",
    "save_map(coordinates_path, np.array([i.coordinates for i in voxel_grid.values()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Map from object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(443299, 3, 1)\n",
      "(443299, 512)\n",
      "(443299, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "from utils.clip_mapping_utils import load_map\n",
    "from utils.clip_utils import get_text_feats\n",
    "from utils.mp3dcat import mp3dcat\n",
    "import numpy as np\n",
    "\n",
    "colors = np.array([i.colors for i in voxel_grid.values()])\n",
    "features = np.array([i.features for i in voxel_grid.values()])\n",
    "coordinates = np.array([i.coordinates for i in voxel_grid.values()])\n",
    "\n",
    "print(colors.shape)\n",
    "print(features.shape)\n",
    "print(coordinates.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Map from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([441861, 3, 1])\n",
      "torch.Size([441861, 512])\n",
      "torch.Size([441861, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "from utils.clip_mapping_utils import load_map\n",
    "from utils.clip_utils import get_text_feats\n",
    "from utils.mp3dcat import mp3dcat\n",
    "import numpy as np\n",
    "\n",
    "colors = torch.tensor(load_map(colors_path))\n",
    "features = torch.tensor(load_map(features_path), dtype=torch.float32)\n",
    "coordinates = torch.tensor(load_map(coordinates_path), dtype=torch.int)\n",
    "\n",
    "print(colors.shape)\n",
    "print(features.shape)\n",
    "print(coordinates.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 416/416 [02:30<00:00,  2.77it/s]\n"
     ]
    }
   ],
   "source": [
    "min_v, _ = torch.min(coordinates, dim=0)\n",
    "\n",
    "coordinates = coordinates - min_v\n",
    "\n",
    "max_v, _ = torch.max(coordinates, dim=0)\n",
    "max_v += 1\n",
    "\n",
    "grid = torch.zeros((max_v[0]+1, max_v[1]+1, max_v[2]+1, 1), dtype=torch.int32)\n",
    "\n",
    "for i, coor in enumerate(coordinates):\n",
    "    grid[coor[0], coor[1], coor[2]] = i\n",
    "\n",
    "kernel_size = 3\n",
    "\n",
    "new_coord = np.zeros((features.shape[0],3))\n",
    "new_feat = np.zeros((features.shape))\n",
    "\n",
    "index = 0\n",
    "kernel_bound = int(kernel_size/2)\n",
    "for x in tqdm(range(grid.shape[0]-kernel_bound)):\n",
    "    for y in range(grid.shape[1]-kernel_bound):\n",
    "        for z in range(grid.shape[2]-kernel_bound):\n",
    "            if(grid[x,y,z] == 0):\n",
    "                continue\n",
    "            lower_x = 0 if x == 0 else x - kernel_bound\n",
    "            lower_y = 0 if y == 0 else y - kernel_bound\n",
    "            lower_z = 0 if z == 0 else z - kernel_bound\n",
    "            voxel_patch = grid[lower_x:x+kernel_bound+1, lower_y:y+kernel_bound+1, lower_z:z+kernel_bound+1]\n",
    "            voxel_patch_feat = features[voxel_patch.flatten()[voxel_patch.flatten() != 0]]\n",
    "\n",
    "            new_feat[index] = torch.mean(voxel_patch_feat, dim=0)\n",
    "            new_coord[index] = [x,y,z]\n",
    "            index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"sofa, cealing, cushion, floor, other\"\n",
    "# lang = mp3dcat\n",
    "lang = lang.split(\",\")\n",
    "text_feats = get_text_feats(lang, clip_model, clip_feat_dim)\n",
    "\n",
    "scores_list = new_feat @ text_feats.T\n",
    "\n",
    "predicts = np.argmax(scores_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang = \"sofa, plant, towel, sink, food, other\"\n",
    "lang = \"sofa, cealing, cushion, floor, other\"\n",
    "lang = lang.split(\",\")\n",
    "text_feats = get_text_feats(lang, clip_model, clip_feat_dim)\n",
    "\n",
    "scores_list = features @ text_feats.T\n",
    "\n",
    "predicts = np.argmax(scores_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = mp3dcat # lang is all the matterport classes\n",
    "text_feats = get_text_feats(lang, clip_model, clip_feat_dim)\n",
    "\n",
    "scores_list = features @ text_feats.T\n",
    "\n",
    "predicts = np.argmax(scores_list, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASIAAACcCAYAAADbCaYHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAARdUlEQVR4nO3de3CU1f3H8fcacwM2Al4KgZBbk5hNE0JNYsPOmCjRRChKHQm2dEaUUsrWIjcNiLkQFDqYCCgmRlACSAdQblJaETJcZE0pt8QoIdUAhmoUbSkY4QeB8PuDcadbLkKy4STk85phxn3OeU6+z874mXOe3X2O5dy5c+cQETHoBtMFiIgoiETEOAWRiBinIBIR4xREImKcgkhEjFMQiYhxCiIRMe5G0wW0R8caw0yXIGLUTd4HPDqeZkQiYpyCSESMUxCJiHEKIhExTkEkIsYpiETEuHYZRKWlpXTt2tV0GSLiIW0+iEJCQpgzZ47pMkSkFbX5ILqWGhsbTZcg0iEZD6KVK1cSExODr68vISEhFBYWutpSU1P57LPPGD9+PBaLBYvF4nbuhg0biI6OpkuXLmRkZFBfX+/WvmDBAqKjo/Hz8+P222+nqKjI1Xbo0CEsFgvLly8nJSUFPz8/li5d2roXKyIXZTSIdu/eTWZmJo888ghVVVXk5eWRnZ1NaWkpAKtWraJ3797k5+dTX1/vFjQnTpygoKCAJUuWsG3bNurq6pg0aZKrfenSpeTk5PD8889TXV3NjBkzyM7OZtGiRW41TJ48mSeffJLq6mrS09OvyXWLiDujvzV78cUXGTBgANnZ2QBERkayb98+XnjhBUaMGEH37t3x8vLCarXSo0cPt3MbGxt59dVXCQ8PB+CJJ54gPz/f1Z6bm0thYSEPPfQQAKGhoezbt4+SkhIeffRRV79x48a5+oiIGUZnRNXV1djtdrdjdrudTz75hLNnz1723E6dOrlCCKBnz54cOXIEgO+++47a2lpGjhxJly5dXP+ee+45amtr3cZJSEjw0NWISHO121/fe3t7u722WCx8vzNSQ0MDAPPnz+fOO+906+fl5eX2unPnzq1YpYhcCaNBFB0djdPpdDvmdDqJjIx0BYaPj88Pzo7+149+9CMCAwM5cOAAw4cP91i9ItI6jAbRxIkTSUxMZPr06QwbNozy8nLmzZvn9ulWSEgI27Zt45FHHsHX15dbbrnlisaeNm0aY8eO5aabbiIjI4NTp06xa9cujh49yoQJE1rrkkSkGYzeI/rpT3/KihUrWLZsGT/5yU/IyckhPz+fESNGuPrk5+dz6NAhwsPDufXWW6947N/85jcsWLCAhQsXEhsbS0pKCqWlpYSGhrbClYhIS1i05fTV0xMapaPTExpF5LqjIBIR4xREImKcgkhEjNPNahExTjMiETFOQSQiximIRMQ4BZGIGKcgEhHj2u1jQEz6v+K7TZcgYpTfmM0eHU8zIhExTkEkIsYpiETEOAWRiBinIBIR49psEK1Zs4Yf//jHeHl5MW7cOO13L3Ida7NBNHr0aB5++GEOHz7M9OnTTZcjIq2oTX6PqKGhgSNHjpCenk5gYOA1+7unT5/Gx8fnmv09ETmvzc2ItmzZgtVqBeCee+7BYrGwZcuWi/YtLi4mPDwcHx8foqKiWLJkiVt7XV0dDz74IF26dCEgIIDMzEy++uorV3teXh7x8fEsWLCA0NBQ/Pz8Wu26ROTS2lwQ9e/fn5qaGgBWrlxJfX09/fv3v6Df6tWrefLJJ5k4cSIfffQRo0eP5rHHHmPz5vPf+GxqauLBBx/k3//+N1u3bmXjxo0cOHCAYcOGuY3z6aefsnLlSlatWkVFRUWrX5+IXKjNLc18fHy47bbbAOjevfsFe95/r6CggBEjRuBwOACYMGECf/vb3ygoKODuu++mrKyMqqoqDh48SFBQEACLFy8mJiaGnTt3kpiYCJxfji1evPiqtioSEc9qczOiK1VdXY3dbnc7Zrfbqa6udrUHBQW5QgjAZrPRtWtXVx+A4OBghZCIYe02iDylc+fOpksQ6fDabRBFR0fjdDrdjjmdTmw2m6v98OHDHD582NW+b98+/vOf/7j6iEjb0ObuEV2pp556iszMTPr160daWhrr1q1j1apVbNq0CYC0tDRiY2MZPnw4c+bM4cyZMzgcDlJSUkhISDBcvYj8t3Y7IxoyZAhz586loKCAmJgYSkpKWLhwIampqQBYLBbWrl1Lt27duOuuu0hLSyMsLIzly5ebLVxELqDthJpBD0aTjk4PRhOR646CSESMUxCJiHEKIhExTkEkIsbpUzMRMU4zIhExTkEkIsYpiETEOAWRiBjXbn/0atLujCmmSxAx6o53Z3p0PM2IRMQ4BZGIGKcgEhHjFEQiYpyCSESMUxCJiHFtOohCQkKYM2fOJdsPHTqExWLRxogi7Vy7/h5RUFAQ9fX13HLLLaZLEZEWaNdB5OXldcmdYEWk/Wjx0szpdJKamkqnTp3o1q0b6enpHD16FLj40io+Pp68vDwAzp07R15eHn369MHX15fAwEDGjh3r1v/EiRM8/vjjWK1W+vTpw2uvveZqu9jSbOvWrSQlJeHr60vPnj2ZPHkyZ86ccbWnpqYyduxYnn76adeW1t/XIyJmtCiIKioqGDBgADabjfLycrZv387gwYM5e/bsFZ2/cuVKZs+eTUlJCZ988glr1qwhNjbWrU9hYSEJCQns3bsXh8PBmDFjqKmpueh4n3/+OQMHDiQxMZHKykqKi4t5/fXXee6559z6LVq0iM6dO7Njxw5mzZpFfn4+GzdubN6bICIt1qKl2axZs0hISKCoqMh1LCYm5orPr6uro0ePHqSlpeHt7U2fPn1ISkpy6zNw4EAcDgcAWVlZzJ49m82bNxMVFXXBeEVFRQQFBTFv3jwsFgu33347X3zxBVlZWeTk5HDDDedzNy4ujtzcXAAiIiKYN28eZWVl3HvvvVf9HohIy3lkRtRcQ4cO5eTJk4SFhTFq1ChWr17ttoyC86HxPYvFQo8ePThy5MhFx6uuriY5ORmLxeI6ZrfbaWho4J///OdFxwTo2bPnJccUkdbXoiDy9/e//OA33MD/Pom2sbHR9d9BQUHU1NRQVFSEv78/DoeDu+66y62Pt7e32/kWi4WmpqaWlN0qY4pI87UoiOLi4igrK7tk+6233kp9fb3r9fHjxzl48KBbH39/fwYPHsxLL73Eli1bKC8vp6qqqln1REdHU15e7hZ+TqcTq9VK7969mzWmiLS+FgXRlClT2LlzJw6Hgw8//JD9+/dTXFzMN998A8A999zDkiVLeP/996mqquLRRx/Fy8vLdX5paSmvv/46H330EQcOHODNN9/E39+f4ODgZtXjcDg4fPgwf/jDH9i/fz9r164lNzeXCRMmuO4PiUjb06L/OyMjI3nvvfeorKwkKSmJ5ORk1q5dy403nr8HPmXKFFJSUvj5z3/OoEGDGDJkCOHh4a7zu3btyvz587Hb7cTFxbFp0ybWrVvHzTff3Kx6evXqxV/+8hf+/ve/07dvX373u98xcuRInn322ZZcpoi0Mm0n1Ax6QqN0dHpCo4hcdxREImKcgkhEjFMQiYhxulktIsZpRiQiximIRMQ4BZGIGKcgEhHjFEQiYly7fma1KTk3LjddgohR+WeGeXQ8zYhExDgFkYgYpyASEeMURCJinIJIRIxrV0E0YsQIhgwZ4nqdmprKuHHjjNUjIp7Rrj++X7Vq1QU7cohI+9Oug6h79+6mSxARD/Do0szpdJKamkqnTp3o1q0b6enpHD16FICmpiZmzpxJaGgo/v7+9O3bl7ffftt17tmzZxk5cqSrPSoqirlz51727/3v0iwkJIQZM2bw+OOPY7Va6dOnD6+99prbOR988AHx8fH4+fmRkJDAmjVrsFgsVFRUeOx9EJGr47Eg+n7XV5vNRnl5Odu3b2fw4MGcPXsWgJkzZ7J48WJeffVVPv74Y8aPH8+vf/1rtm7dCpwPqt69e/PWW2+xb98+cnJyeOaZZ1ixYsVV1VFYWEhCQgJ79+7F4XAwZswYampqgPP7qg0ePJjY2Fj27NnD9OnTycrK8tRbICLN5LGl2axZs0hISKCoqMh1LCYmBoBTp04xY8YMNm3aRHJyMgBhYWFs376dkpISUlJS8Pb2Ztq0aa5zQ0NDKS8vZ8WKFWRmZl5xHQMHDsThcACQlZXF7Nmz2bx5M1FRUfzpT3/CYrEwf/58/Pz8sNlsfP7554waNcoTb4GINJPHgqiiooKhQ4detO3TTz/lxIkT3HvvvW7HT58+Tb9+/VyvX3nlFd544w3q6uo4efIkp0+fJj4+/qrq+O997S0WCz169HDta19TU0NcXBx+fn6uPklJSVc1voh4nseCyN/f/5JtDQ0NAKxfv55evXq5tfn6+gKwbNkyJk2aRGFhIcnJyVitVl544QV27NhxVXVoX3uR9sdjQRQXF0dZWZnb8up7NpsNX19f6urqSElJuej5TqeT/v37u5ZVALW1tZ4qD4CoqCjefPNNTp065QrAnTt3evRviMjV89jN6ilTprBz504cDgcffvgh+/fvp7i4mG+++Qar1cqkSZMYP348ixYtora2lj179vDyyy+zaNEiACIiIti1axcbNmzgH//4B9nZ2R4PiV/96lc0NTXx29/+lurqajZs2EBBQQFwfuYkImZ4LIgiIyN57733qKysJCkpieTkZNauXcuNN56fdE2fPp3s7GxmzpxJdHQ0GRkZrF+/ntDQUABGjx7NQw89xLBhw7jzzjv517/+5TY78oSAgADWrVtHRUUF8fHxTJ06lZycHAC3+0Yicm11+O2Eli5dymOPPcaxY8cue5/rv+nBaNLRefrBaO36m9XNsXjxYsLCwujVqxeVlZVkZWWRmZl5xSEkIp7X4YLoyy+/JCcnhy+//JKePXsydOhQnn/+edNliXRoHX5p1hxamklHp2dWi8h1R0EkIsZpaSYixmlGJCLGKYhExDgFkYgYpyASEeM63BcaPSHA/4+mSxAx6vjJyR4dTzMiETFOQSQiximIRMQ4BZGIGKcgEhHjFEQiYtx1HUT79+/nZz/7GX5+fle9LZGIXDvX9feIcnNz6dy5MzU1NXTp0sV0OSJyCdd1ENXW1jJo0CCCg4NNlyIil9Hml2Zvv/02sbGx+Pv7c/PNN5OWlsZ3331HU1MT+fn59O7dG19fX+Lj43n33Xdd51ksFnbv3k1+fj4Wi4W8vDzg/DbUkZGRdOrUibCwMLKzs2lsbDR0dSICbXxGVF9fzy9/+UtmzZrFL37xC7799lvef/99zp07x9y5cyksLKSkpIR+/frxxhtv8MADD/Dxxx8TERFBfX09aWlpZGRkMGnSJNfSzGq1UlpaSmBgIFVVVYwaNQqr1crTTz9t+GpFOq42/WC0PXv2cMcdd3Do0KELlle9evXi97//Pc8884zrWFJSEomJibzyyisAxMfHM2TIENds6GIKCgpYtmwZu3btuuK69Fsz6eg8/VuzNj0j6tu3LwMGDCA2Npb09HTuu+8+Hn74Yby8vPjiiy+w2+1u/e12O5WVlZcdc/ny5bz00kvU1tbS0NDAmTNnCAgIaM3LEJEf0KbvEXl5ebFx40b++te/YrPZePnll4mKiuLgwYPNGq+8vJzhw4czcOBA/vznP7N3716mTp3K6dOnPVy5iFyNNh1EcP6ms91uZ9q0aezduxcfHx/KysoIDAzE6XS69XU6ndhstkuO9cEHHxAcHMzUqVNJSEggIiKCzz77rLUvQUR+QJtemu3YsYOysjLuu+8+brvtNnbs2MHXX39NdHQ0Tz31FLm5uYSHhxMfH8/ChQupqKhg6dKllxwvIiKCuro6li1bRmJiIuvXr2f16tXX8IpE5GLadBAFBASwbds25syZw/HjxwkODqawsJD777+f9PR0jh07xsSJEzly5Ag2m4133nmHiIiIS473wAMPMH78eJ544glOnTrFoEGDyM7OvuzNbBFpfW36U7O2Sp+aSUenJzSKyHVHQSQiximIRMQ4BZGIGKeb1SJinGZEImKcgkhEjFMQiYhxCiIRMU5BJCLGKYhExDgFkYgYpyASEeMURCJi3P8DV5o3si4U83wAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 300x166.667 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_categories = len(lang)\n",
    "\n",
    "# Choose a colormap (e.g., inferno, plasma, viridis)\n",
    "cmap = cm.plasma\n",
    "\n",
    "# Generate an array of values from 0 to 1 representing the colormap index\n",
    "values = np.linspace(0, 1, num_categories)\n",
    "\n",
    "# Get RGB colors from the colormap for each value\n",
    "color_palette = cmap(values)[:,0:3]\n",
    "\n",
    "# round colors\n",
    "color_palette = np.round(color_palette, 2)\n",
    "\n",
    "plt.figure(figsize=(3,num_categories/3))\n",
    "bars = plt.barh(lang, [1 for i in range(num_categories)], color=color_palette)\n",
    "\n",
    "plt.gca().xaxis.set_visible(False)\n",
    "plt.margins(y=0.0)\n",
    "plt.box(False)\n",
    "plt.tick_params(left=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Segmented Voxel Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.open3d import create_voxel\n",
    "\n",
    "voxel_grid = create_voxel(new_coord, predicts, color_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.open3d import create_voxel\n",
    "\n",
    "voxel_grid = create_voxel(coordinates, predicts, color_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Real Voxel Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.open3d import create_real_rgb_voxel\n",
    "\n",
    "voxel_grid = create_real_rgb_voxel(coordinates, colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'create_white_voxel' from 'utils.open3d' (/Users/anthony/Documents/UW/Classes/SP24/CSE_571_Robotics/Project/vlmaps/utils/open3d.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopen3d\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_white_voxel\n\u001b[1;32m      3\u001b[0m voxel_grid \u001b[38;5;241m=\u001b[39m create_white_voxel(coordinates)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'create_white_voxel' from 'utils.open3d' (/Users/anthony/Documents/UW/Classes/SP24/CSE_571_Robotics/Project/vlmaps/utils/open3d.py)"
     ]
    }
   ],
   "source": [
    "from utils.open3d import create_white_voxel\n",
    "\n",
    "voxel_grid = create_white_voxel(coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Voxel Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[Open3D WARNING] GLFW Error: Cocoa: Failed to find service port for display\u001b[0;m\n"
     ]
    }
   ],
   "source": [
    "from utils.open3d import visualize_map\n",
    "\n",
    "visualize_map(voxel_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "6db63e020538e79a80b9b328e157333fe21136baff2d1659d3fa8a4dfb8ecd33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
