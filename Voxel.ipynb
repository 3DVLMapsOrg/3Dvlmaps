{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0m1nbfGay6R"
   },
   "source": [
    "## VLMap Creation\n",
    "It takes around 20 minutes to build a VLMap with around 1000 RGBD frames. We also provide a pre-built VLMap. Skip to the Landmark Indexing part of the code to directly try our map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bx_EfAUoay6S"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "data_dir = r\"C:\\Users\\Andrew Jeon\\OneDrive\\Desktop\\vlmaps\\data\\5LpN3gDmAk7_1\"\n",
    "\n",
    "# setup parameters\n",
    "# @markdown meters per cell size\n",
    "cs = 0.05 # @param {type: \"number\"}\n",
    "VOXEL_SIZE = 0.05 # @param {type: \"number\"}\n",
    "# @markdown map resolution (gs x gs)\n",
    "gs = 1000 # @param {type: \"integer\"}\n",
    "# @markdown camera height (used for filtering out points on the floor)\n",
    "camera_height = 1.5 # @param {type: \"number\"}\n",
    "# @markdown depth pixels subsample rate\n",
    "depth_sample_rate = 100 # @param {type: \"integer\"}\n",
    "# @markdown data where rgb, depth, pose are loaded and map are saved\n",
    "data_dir = data_dir # @param {type: \"string\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "from utils.clip_mapping_utils import load_pose, load_semantic, load_obj2cls_dict, save_map, cvt_obj_id_2_cls_id, depth2pc, transform_pc, get_sim_cam_mat, pos2grid_id, project_point\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_depth(depth_filepath):\n",
    "    with open(depth_filepath, 'rb') as f:\n",
    "        depth = np.load(f)\n",
    "    return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrew Jeon\\OneDrive\\Desktop\\vlmaps\n"
     ]
    }
   ],
   "source": [
    "cd \"C:\\Users\\Andrew Jeon\\OneDrive\\Desktop\\vlmaps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Loading CLIP model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrew Jeon\\Anaconda3\\envs\\vlmaps\\lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading scene C:\\Users\\Andrew Jeon\\OneDrive\\Desktop\\vlmaps\\data\\5LpN3gDmAk7_1\n",
      "C:\\Users\\Andrew Jeon\\OneDrive\\Desktop\\vlmaps\\data\\5LpN3gDmAk7_1\\map\\color_top_down_1.npy is saved.\n",
      "C:\\Users\\Andrew Jeon\\OneDrive\\Desktop\\vlmaps\\data\\5LpN3gDmAk7_1\\map\\grid_1_gt.npy is saved.\n",
      "C:\\Users\\Andrew Jeon\\OneDrive\\Desktop\\vlmaps\\data\\5LpN3gDmAk7_1\\map\\grid_lseg_1.npy is saved.\n",
      "C:\\Users\\Andrew Jeon\\OneDrive\\Desktop\\vlmaps\\data\\5LpN3gDmAk7_1\\map\\weight_lseg_1.npy is saved.\n",
      "C:\\Users\\Andrew Jeon\\OneDrive\\Desktop\\vlmaps\\data\\5LpN3gDmAk7_1\\map\\obstacles.npy is saved.\n"
     ]
    }
   ],
   "source": [
    "from lseg.modules.models.lseg_net import LSegEncNet\n",
    "from lseg.additional_utils.models import resize_image, pad_image, crop_image\n",
    "import clip\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "img_save_dir = data_dir\n",
    "mask_version = 1\n",
    "crop_size = 480 # 480\n",
    "base_size = 520 # 520\n",
    "lang = \"door,chair,ground,ceiling,other\"\n",
    "labels = lang.split(\",\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "clip_version = \"ViT-B/32\"\n",
    "clip_feat_dim = {'RN50': 1024, 'RN101': 512, 'RN50x4': 640, 'RN50x16': 768,\n",
    "                'RN50x64': 1024, 'ViT-B/32': 512, 'ViT-B/16': 512, 'ViT-L/14': 768}[clip_version]\n",
    "\n",
    "print(\"Loading CLIP model...\")\n",
    "clip_model, preprocess = clip.load(clip_version)  # clip.available_models()\n",
    "clip_model.to(device).eval()\n",
    "lang_token = clip.tokenize(labels)\n",
    "lang_token = lang_token.to(device)\n",
    "with torch.no_grad():\n",
    "    text_feats = clip_model.encode_text(lang_token)\n",
    "    text_feats = text_feats / text_feats.norm(dim=-1, keepdim=True)\n",
    "text_feats = text_feats.cpu().numpy()\n",
    "model = LSegEncNet(lang, arch_option=0,\n",
    "                    block_depth=0,\n",
    "                    activation='lrelu',\n",
    "                    crop_size=crop_size)\n",
    "model_state_dict = model.state_dict()\n",
    "pretrained_state_dict = torch.load(\"lseg/checkpoints/demo_e200.ckpt\")\n",
    "pretrained_state_dict = {k.lstrip('net.'): v for k, v in pretrained_state_dict['state_dict'].items()}\n",
    "model_state_dict.update(pretrained_state_dict)\n",
    "model.load_state_dict(pretrained_state_dict)\n",
    "\n",
    "model.eval()\n",
    "model = model.cuda()\n",
    "\n",
    "norm_mean= [0.5, 0.5, 0.5]\n",
    "norm_std = [0.5, 0.5, 0.5]\n",
    "padding = [0.0] * 3\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"loading scene {img_save_dir}\")\n",
    "rgb_dir = os.path.join(img_save_dir, \"rgb\")\n",
    "depth_dir = os.path.join(img_save_dir, \"depth\")\n",
    "pose_dir = os.path.join(img_save_dir, \"pose\")\n",
    "semantic_dir = os.path.join(img_save_dir, \"semantic\")\n",
    "obj2cls_path = os.path.join(img_save_dir, \"obj2cls_dict.txt\")\n",
    "\n",
    "rgb_list = sorted(os.listdir(rgb_dir), key=lambda x: int(\n",
    "    x.split(\"_\")[-1].split(\".\")[0]))\n",
    "depth_list = sorted(os.listdir(depth_dir), key=lambda x: int(\n",
    "    x.split(\"_\")[-1].split(\".\")[0]))\n",
    "pose_list = sorted(os.listdir(pose_dir), key=lambda x: int(\n",
    "    x.split(\"_\")[-1].split(\".\")[0]))\n",
    "pose_list = sorted(os.listdir(pose_dir), key=lambda x: int(\n",
    "    x.split(\"_\")[-1].split(\".\")[0]))\n",
    "semantic_list = sorted(os.listdir(semantic_dir), key=lambda x: int(\n",
    "    x.split(\"_\")[-1].split(\".\")[0]))\n",
    "\n",
    "rgb_list = [os.path.join(rgb_dir, x) for x in rgb_list]\n",
    "depth_list = [os.path.join(depth_dir, x) for x in depth_list]\n",
    "pose_list = [os.path.join(pose_dir, x) for x in pose_list]\n",
    "semantic_list = [os.path.join(semantic_dir, x) for x in semantic_list]\n",
    "\n",
    "\n",
    "map_save_dir = os.path.join(img_save_dir, \"map\")\n",
    "os.makedirs(map_save_dir, exist_ok=True)\n",
    "color_top_down_save_path = os.path.join(map_save_dir, f\"color_top_down_{mask_version}.npy\")\n",
    "gt_save_path = os.path.join(map_save_dir, f\"grid_{mask_version}_gt.npy\")\n",
    "grid_save_path = os.path.join(map_save_dir, f\"grid_lseg_{mask_version}.npy\")\n",
    "weight_save_path = os.path.join(map_save_dir, f\"weight_lseg_{mask_version}.npy\")\n",
    "obstacles_save_path = os.path.join(map_save_dir, \"obstacles.npy\")\n",
    "\n",
    "obj2cls = load_obj2cls_dict(obj2cls_path)\n",
    "\n",
    "# initialize a grid with zero position at the center\n",
    "color_top_down_height = (camera_height + 1) * np.ones((gs, gs), dtype=np.float32)\n",
    "color_top_down = np.zeros((gs, gs, 3), dtype=np.uint8)\n",
    "gt = np.zeros((gs, gs), dtype=np.int32)\n",
    "grid = np.zeros((gs, gs, clip_feat_dim), dtype=np.float32)\n",
    "obstacles = np.ones((gs, gs), dtype=np.uint8)\n",
    "weight = np.zeros((gs, gs), dtype=float)\n",
    "\n",
    "save_map(color_top_down_save_path, color_top_down)\n",
    "save_map(gt_save_path, gt)\n",
    "save_map(grid_save_path, grid)\n",
    "save_map(weight_save_path, weight)\n",
    "save_map(obstacles_save_path, obstacles)\n",
    "\n",
    "tf_list = []\n",
    "data_iter = zip(rgb_list, depth_list, semantic_list, pose_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetVoxelCoor(point):\n",
    "    return ((point) / VOXEL_SIZE).astype(int)\n",
    "\n",
    "def GetVoxelId(point, min_bound_voxel_grid):\n",
    "    return ((point - min_bound_voxel_grid) / VOXEL_SIZE).astype(int) # returns 3, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Voxel:\n",
    "    def __init__(self, coord, feature_shape) -> None:\n",
    "        self.coordinates = coord\n",
    "        self.feature_weight = 0\n",
    "        self.color_weight = 0\n",
    "        self.features = np.zeros(feature_shape)\n",
    "        self.colors = np.zeros((3,1))\n",
    "        self.class_mask = 0\n",
    "\n",
    "    def update_color(self, color):\n",
    "        self.colors = (self.colors * self.color_weight + color) / (self.color_weight + 1)\n",
    "        self.color_weight += 1\n",
    "\n",
    "    def update_feature(self, feature):\n",
    "        \"\"\"\n",
    "        Updates voxel features with weighted average.\n",
    "\n",
    "        Args:\n",
    "            weight: A float representing the weight for this voxel.\n",
    "            feature: A NumPy array with the same shape as self.features \n",
    "            representing the feature to be added.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.features.shape != feature.shape:\n",
    "            raise ValueError(\"Feature shape must match voxel feature shape\")\n",
    "\n",
    "        # Weighted average update\n",
    "        self.features = (self.features * self.feature_weight + feature) / (self.feature_weight + 1)\n",
    "        self.feature_weight += 1\n",
    "\n",
    "        \n",
    "    def expectedColor(self):\n",
    "        return self.sum / self.pc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This thing outputs a image segmentation mask as a np array\n",
    "def get_lseg_feat(model: LSegEncNet, image: np.array, labels, transform, crop_size=480, \\\n",
    "                 base_size=520, norm_mean=[0.5, 0.5, 0.5], norm_std=[0.5, 0.5, 0.5]):\n",
    "    vis_image = image.copy()\n",
    "    image = transform(image).unsqueeze(0).cuda() # adds 1 dimension at the start of the np array\n",
    "    img = image[0].permute(1,2,0) # switches the order of the dimensions of the image\n",
    "    img = img * 0.5 + 0.5\n",
    "    \n",
    "    batch, _, h, w = image.size() # batch is dimension 1, ignoring channel, y, x\n",
    "    stride_rate = 2.0/3.0\n",
    "    stride = int(crop_size * stride_rate)\n",
    "\n",
    "    long_size = base_size\n",
    "    if h > w:\n",
    "        height = long_size\n",
    "        width = int(1.0 * w * long_size / h + 0.5)\n",
    "        short_size = width\n",
    "    else:\n",
    "        width = long_size\n",
    "        height = int(1.0 * h * long_size / w + 0.5)\n",
    "        short_size = height\n",
    "\n",
    "\n",
    "    cur_img = resize_image(image, height, width, **{'mode': 'bilinear', 'align_corners': True})\n",
    "\n",
    "    if long_size <= crop_size:\n",
    "        pad_img = pad_image(cur_img, norm_mean,\n",
    "                            norm_std, crop_size)\n",
    "        print(pad_img.shape)\n",
    "        with torch.no_grad():\n",
    "            outputs, logits = model(pad_img, labels)\n",
    "        outputs = crop_image(outputs, 0, height, 0, width)\n",
    "    else:\n",
    "        if short_size < crop_size:\n",
    "            # pad if needed\n",
    "            pad_img = pad_image(cur_img, norm_mean,\n",
    "                                norm_std, crop_size)\n",
    "        else:\n",
    "            pad_img = cur_img\n",
    "        _,_,ph,pw = pad_img.shape #.size()\n",
    "        assert(ph >= height and pw >= width)\n",
    "        h_grids = int(math.ceil(1.0 * (ph-crop_size)/stride)) + 1\n",
    "        w_grids = int(math.ceil(1.0 * (pw-crop_size)/stride)) + 1\n",
    "        with torch.cuda.device_of(image):\n",
    "            with torch.no_grad():\n",
    "                outputs = image.new().resize_(batch, model.out_c,ph,pw).zero_().cuda()\n",
    "                logits_outputs = image.new().resize_(batch, len(labels),ph,pw).zero_().cuda()\n",
    "            count_norm = image.new().resize_(batch,1,ph,pw).zero_().cuda()\n",
    "        # grid evaluation\n",
    "        for idh in range(h_grids):\n",
    "            for idw in range(w_grids):\n",
    "                h0 = idh * stride\n",
    "                w0 = idw * stride\n",
    "                h1 = min(h0 + crop_size, ph)\n",
    "                w1 = min(w0 + crop_size, pw)\n",
    "                crop_img = crop_image(pad_img, h0, h1, w0, w1)\n",
    "                # pad if needed\n",
    "                pad_crop_img = pad_image(crop_img, norm_mean,\n",
    "                                            norm_std, crop_size)\n",
    "                with torch.no_grad():\n",
    "                    output, logits = model(pad_crop_img, labels)\n",
    "                cropped = crop_image(output, 0, h1-h0, 0, w1-w0)\n",
    "                cropped_logits = crop_image(logits, 0, h1-h0, 0, w1-w0)\n",
    "                outputs[:,:,h0:h1,w0:w1] += cropped\n",
    "                logits_outputs[:,:,h0:h1,w0:w1] += cropped_logits\n",
    "                count_norm[:,:,h0:h1,w0:w1] += 1\n",
    "        assert((count_norm==0).sum()==0)\n",
    "        outputs = outputs / count_norm\n",
    "        logits_outputs = logits_outputs / count_norm\n",
    "        outputs = outputs[:,:,:height,:width]\n",
    "        logits_outputs = logits_outputs[:,:,:height,:width]\n",
    "    outputs = outputs.cpu()\n",
    "    outputs = outputs.numpy() # B, D, H, W\n",
    "    predicts = [torch.max(logit, 0)[1].cpu().numpy() for logit in logits_outputs]\n",
    "    pred = predicts[0]\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_frames = len(depth_list)\n",
    "# resolution = 720 * 1080\n",
    "# channels = 3 #RGB\n",
    "# dimensions = 3 #XYZ\n",
    "import math\n",
    "\n",
    "\n",
    "voxel_grid = {}\n",
    "\n",
    "for frame_index, data_sample in enumerate(data_iter):    \n",
    "    rgb_path, depth_path, semantic_path, pose_path = data_sample\n",
    "    \n",
    "    bgr = cv2.imread(rgb_path)\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # read pose\n",
    "    pos, rot = load_pose(pose_path)  # z backward, y upward, x to the right\n",
    "    rot_ro_cam = np.eye(3)\n",
    "    rot_ro_cam[1, 1] = -1\n",
    "    rot_ro_cam[2, 2] = -1\n",
    "    rot = rot @ rot_ro_cam\n",
    "    pos[1] += camera_height\n",
    "\n",
    "\n",
    "    pose = np.eye(4)\n",
    "    pose[:3, :3] = rot\n",
    "    pose[:3, 3] = pos.reshape(-1)\n",
    "\n",
    "    tf_list.append(pose)\n",
    "    if len(tf_list) == 1:\n",
    "        init_tf_inv = np.linalg.inv(tf_list[0]) \n",
    "\n",
    "    tf = init_tf_inv @ pose\n",
    "\n",
    "    # read depth\n",
    "    depth = load_depth(depth_path)\n",
    "\n",
    "    # read semantic\n",
    "    semantic = load_semantic(semantic_path)\n",
    "    semantic = cvt_obj_id_2_cls_id(semantic, obj2cls)\n",
    "\n",
    "    #TODO\n",
    "    pix_feats = get_lseg_feat(model, rgb, labels, transform, crop_size, base_size, norm_mean, norm_std)\n",
    "    \n",
    "    # transform all points to the global frame\n",
    "    pc, mask = depth2pc(depth)\n",
    "    shuffle_mask = np.arange(pc.shape[1]) \n",
    "    np.random.shuffle(shuffle_mask)\n",
    "    shuffle_mask = shuffle_mask[::depth_sample_rate]\n",
    "    mask = mask[shuffle_mask]\n",
    "    pc = pc[:, shuffle_mask]\n",
    "    pc = pc[:, mask]\n",
    "    pc_global = transform_pc(pc, tf)\n",
    "\n",
    "    rgb_cam_mat = get_sim_cam_mat(rgb.shape[0], rgb.shape[1])\n",
    "    #TODO\n",
    "    feat_cam_mat = get_sim_cam_mat(pix_feats.shape[2], pix_feats.shape[3])\n",
    "\n",
    "    # project all point cloud onto the ground\n",
    "    for pixel_index, (p, p_local) in enumerate(zip(pc_global.T, pc.T)):\n",
    "    \n",
    "        x, y = pos2grid_id(gs, cs, p[0], p[2])\n",
    "\n",
    "        single_global_point = (tf @ np.vstack([p_local.reshape(3,1), np.ones((1, 1))]) )[:3]\n",
    "\n",
    "        rgb_px, rgb_py, rgb_pz = project_point(rgb_cam_mat, p_local)\n",
    "        rgb_v = rgb[rgb_py, rgb_px, :].reshape(3,1)\n",
    "\n",
    "        semantic_v = semantic[rgb_py, rgb_px] # this calculates the class id of the pixel, which is used to build the Ground Truth map\n",
    "        if semantic_v == 40:\n",
    "            semantic_v = -1\n",
    "\n",
    "        # if not (px < 0 or py < 0 or px >= pix_feats.shape[3] or py >= pix_feats.shape[2]):\n",
    "        #     feat = pix_feats[0, :, py, px]   #these are for finding the corresponding features for that pixel in the feature matrix\n",
    "        #     grid[y, x] = (grid[y, x] * weight[y, x] + feat) / (weight[y, x] + 1) # grid[y,x] refers to value stored at current grid cell location whicih is likely the accumulated feature information, \n",
    "        #     # weight refers to how many points have contribute to current value of grid cell, \n",
    "        #     # feat is the LSEG feature embedding exxtracted for current point being processed\n",
    "        #     # feat is added to existing features in cell, weighted by number of points already there weight[y,x]\n",
    "        #     weight[y, x] += 1\n",
    "\n",
    "        voxel_coor = GetVoxelCoor(single_global_point)\n",
    "        voxel_key = str(voxel_coor[0,0]) + \",\" + str(voxel_coor[1,0]) + \",\" + str(voxel_coor[2,0])\n",
    "        if voxel_key not in voxel_grid:\n",
    "            voxel_grid[voxel_key] = Voxel(voxel_coor, pix_feats.shape[1]) #if not in voxel grid, then make it\n",
    "        voxel_grid[voxel_key].update_color((rgb_v/ 255.0))\n",
    "        \n",
    "\n",
    "        # average the visual embeddings if multiple points are projected to the same grid cell\n",
    "        # we want to change this to average each 3D voxel  \n",
    "        px, py, pz = project_point(feat_cam_mat, p_local)\n",
    "        if not (px < 0 or py < 0 or px >= pix_feats.shape[3] or py >= pix_feats.shape[2]):\n",
    "            feat = pix_feats[0, :, py, px] #these are for finding the corresponding features for that pixel in the feature matrix\n",
    "            voxel_grid[voxel_key].update_feature(feat)\n",
    "        # voxel_grid[voxel_key] = (voxel_grid[voxel_key] * weight[y, x] + feat) / (weight[y, x] + 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'voxel_grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m save_map(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAndrew Jeon\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mvlmaps\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmaps\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcolormap.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39marray([i\u001b[38;5;241m.\u001b[39mcolors \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvoxel_grid\u001b[49m\u001b[38;5;241m.\u001b[39mvalues()]))\n\u001b[0;32m      2\u001b[0m save_map(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAndrew Jeon\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mvlmaps\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmaps\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfeaturemap.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39marray([i\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m voxel_grid\u001b[38;5;241m.\u001b[39mvalues()]))\n\u001b[0;32m      3\u001b[0m save_map(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAndrew Jeon\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mvlmaps\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmaps\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcoordinates.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39marray([i\u001b[38;5;241m.\u001b[39mcoordinates \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m voxel_grid\u001b[38;5;241m.\u001b[39mvalues()]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'voxel_grid' is not defined"
     ]
    }
   ],
   "source": [
    "save_map(r\"C:\\Users\\Andrew Jeon\\OneDrive\\Desktop\\vlmaps\\maps\\colormap.npy\", np.array([i.colors for i in voxel_grid.values()]))\n",
    "save_map(r\"C:\\Users\\Andrew Jeon\\OneDrive\\Desktop\\vlmaps\\maps\\featuremap.npy\", np.array([i.features for i in voxel_grid.values()]))\n",
    "save_map(r\"C:\\Users\\Andrew Jeon\\OneDrive\\Desktop\\vlmaps\\maps\\coordinates.npy\", np.array([i.coordinates for i in voxel_grid.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Input the prompt as a string of object names separated by \",\"\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from utils.clip_mapping_utils import load_map, get_new_pallete, get_new_mask_pallete\n",
    "from utils.clip_utils import get_text_feats\n",
    "from utils.mp3dcat import mp3dcat\n",
    "import clip\n",
    "\n",
    "\n",
    "\n",
    "colors = load_map(r\"C:\\Users\\Andrew Jeon\\OneDrive\\Desktop\\vlmaps\\maps\\colormap.npy\")\n",
    "features = load_map(r\"C:\\Users\\Andrew Jeon\\OneDrive\\Desktop\\vlmaps\\maps\\featuremap.npy\")\n",
    "coordinates = load_map(r\"C:\\Users\\Andrew Jeon\\OneDrive\\Desktop\\vlmaps\\maps\\coordinates.npy\")\n",
    "\n",
    "# lang = \"big flat counter, sofa, floor, chair, wash basin, other\" # @param {type: \"string\"}\n",
    "# lang = lang.split(\",\")\n",
    "\n",
    "lang = mp3dcat # lang is all the matterport classes\n",
    "text_feats = get_text_feats(lang, clip_model, clip_feat_dim)\n",
    "\n",
    "map_feats = features\n",
    "# map_feats = [i.features for i in voxel_grid.values()]\n",
    "scores_list = map_feats @ text_feats.T\n",
    "\n",
    "predicts = np.argmax(scores_list, axis=1)\n",
    "\n",
    "# for voxel, class_mask in zip(voxel_grid, predicts):\n",
    "#     voxel.class_mask = class_mask\n",
    "\n",
    "#predicts = predicts.reshape((xmax - xmin + 1, ymax - ymin + 1))\n",
    "# floor_mask = predicts == 2\n",
    "\n",
    "# new_pallete = get_new_pallete(len(lang))\n",
    "\n",
    "# mask, patches = get_new_mask_pallete(predicts, new_pallete, out_label_flag=True, labels=lang)\n",
    "# seg = mask.convert(\"RGBA\")\n",
    "# seg = np.array(seg)\n",
    "# seg[no_map_mask] = [225, 225, 225, 255]\n",
    "# seg[floor_mask] = [225, 225, 225, 255]\n",
    "# seg = Image.fromarray(seg)\n",
    "# plt.figure(figsize=(10, 6), dpi=120)\n",
    "# plt.legend(handles=patches, loc='upper left', bbox_to_anchor=(1., 1), prop={'size': 10})\n",
    "# plt.axis('off')\n",
    "# plt.title(\"VLMaps\")\n",
    "# plt.imshow(seg)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.   0.   0.  ]\n",
      " [0.   1.   0.  ]\n",
      " [0.   0.   1.  ]\n",
      " [1.   0.5  0.  ]\n",
      " [0.5  1.   0.  ]\n",
      " [0.   1.   0.5 ]\n",
      " [0.5  0.   1.  ]\n",
      " [1.   0.   1.  ]\n",
      " [0.7  0.7  0.7 ]\n",
      " [0.3  0.3  0.3 ]\n",
      " [1.   0.8  0.6 ]\n",
      " [0.8  1.   0.8 ]\n",
      " [0.6  0.8  1.  ]\n",
      " [0.9  0.5  0.  ]\n",
      " [0.5  0.9  0.  ]\n",
      " [0.   0.6  0.9 ]\n",
      " [0.9  0.   0.6 ]\n",
      " [1.   1.   0.5 ]\n",
      " [0.2  0.5  0.8 ]\n",
      " [0.8  0.2  0.5 ]\n",
      " [0.5  0.8  0.2 ]\n",
      " [0.7  0.4  0.1 ]\n",
      " [0.1  0.7  0.4 ]\n",
      " [0.4  0.1  0.7 ]\n",
      " [0.   1.   1.  ]\n",
      " [0.   0.5  0.5 ]\n",
      " [0.5  0.   0.5 ]\n",
      " [1.   0.75 0.75]\n",
      " [0.75 1.   0.75]\n",
      " [0.75 0.75 1.  ]\n",
      " [0.25 0.25 0.25]\n",
      " [0.4  0.4  0.4 ]\n",
      " [0.6  0.6  0.6 ]\n",
      " [0.   0.2  0.8 ]\n",
      " [0.8  0.   0.2 ]\n",
      " [0.2  0.8  0.  ]\n",
      " [0.4  0.8  0.4 ]\n",
      " [0.8  0.4  0.4 ]\n",
      " [0.4  0.4  0.8 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from utils.clip_mapping_utils import load_map, get_new_pallete, get_new_mask_pallete\n",
    "from utils.clip_utils import get_text_feats\n",
    "from utils.mp3dcat import mp3dcat\n",
    "import clip\n",
    "\n",
    "\n",
    "color_list = [[1, 0, 0], [0, 1, 0], [0, 0, 1],\n",
    "              [1, 0.5, 0], [0.5, 1, 0], [0, 1, 0.5], [0.5, 0, 1], [1, 0, 1],\n",
    "              [0.7, 0.7, 0.7], [0.3, 0.3, 0.3], [1, 0.8, 0.6], [0.8, 1, 0.8],\n",
    "              [0.6, 0.8, 1], [0.9, 0.5, 0], [0.5, 0.9, 0], [0, 0.6, 0.9],\n",
    "              [0.9, 0, 0.6], [1, 1, 0.5], [0.2, 0.5, 0.8], [0.8, 0.2, 0.5],\n",
    "              [0.5, 0.8, 0.2], [0.7, 0.4, 0.1], [0.1, 0.7, 0.4], [0.4, 0.1, 0.7],\n",
    "              [0, 1, 1], [0, 0.5, 0.5], [0.5, 0, 0.5], [1, 0.75, 0.75],\n",
    "              [0.75, 1, 0.75], [0.75, 0.75, 1], [0.25, 0.25, 0.25], [0.4, 0.4, 0.4],\n",
    "              [0.6, 0.6, 0.6], [0, 0.2, 0.8], [0.8, 0, 0.2], [0.2, 0.8, 0],\n",
    "              [0.4, 0.8, 0.4], [0.8, 0.4, 0.4], [0.4, 0.4, 0.8]]\n",
    "\n",
    "\n",
    "\n",
    "# Convert the list to a NumPy array\n",
    "color_array = np.array(color_list)\n",
    "\n",
    "\n",
    "color_palette = color_array[0:len(lang),:]\n",
    "print(color_palette)\n",
    "\n",
    "\n",
    "# new_pallete = (np.array(new_pallete) / 255.0).reshape(3,6).T\n",
    "# print(new_pallete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443307\n",
      "(443307, 6)\n",
      "(443307,)\n"
     ]
    }
   ],
   "source": [
    "# print(len(voxel_grid.values()))\n",
    "# print(scores_list.shape)\n",
    "# print(predicts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: open3d==0.18.0 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from open3d==0.18.0) (1.24.3)\n",
      "Requirement already satisfied: dash>=2.6.0 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from open3d==0.18.0) (2.17.0)\n",
      "Requirement already satisfied: werkzeug>=2.2.3 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from open3d==0.18.0) (3.0.3)\n",
      "Requirement already satisfied: nbformat>=5.7.0 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from open3d==0.18.0) (5.7.0)\n",
      "Requirement already satisfied: configargparse in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from open3d==0.18.0) (1.7)\n",
      "Requirement already satisfied: ipywidgets>=8.0.4 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from open3d==0.18.0) (8.1.2)\n",
      "Requirement already satisfied: Flask<3.1,>=1.0.4 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from dash>=2.6.0->open3d==0.18.0) (3.0.3)\n",
      "Requirement already satisfied: plotly>=5.0.0 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from dash>=2.6.0->open3d==0.18.0) (5.22.0)\n",
      "Requirement already satisfied: dash-html-components==2.0.0 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from dash>=2.6.0->open3d==0.18.0) (2.0.0)\n",
      "Requirement already satisfied: dash-core-components==2.0.0 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from dash>=2.6.0->open3d==0.18.0) (2.0.0)\n",
      "Requirement already satisfied: dash-table==5.0.0 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from dash>=2.6.0->open3d==0.18.0) (5.0.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from dash>=2.6.0->open3d==0.18.0) (7.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from dash>=2.6.0->open3d==0.18.0) (4.9.0)\n",
      "Requirement already satisfied: requests in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from dash>=2.6.0->open3d==0.18.0) (2.31.0)\n",
      "Requirement already satisfied: retrying in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from dash>=2.6.0->open3d==0.18.0) (1.3.4)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from dash>=2.6.0->open3d==0.18.0) (1.6.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from dash>=2.6.0->open3d==0.18.0) (68.2.2)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from ipywidgets>=8.0.4->open3d==0.18.0) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from ipywidgets>=8.0.4->open3d==0.18.0) (8.12.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from ipywidgets>=8.0.4->open3d==0.18.0) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from ipywidgets>=8.0.4->open3d==0.18.0) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from ipywidgets>=8.0.4->open3d==0.18.0) (3.0.10)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from nbformat>=5.7.0->open3d==0.18.0) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from nbformat>=5.7.0->open3d==0.18.0) (4.19.2)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from nbformat>=5.7.0->open3d==0.18.0) (5.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from werkzeug>=2.2.3->open3d==0.18.0) (2.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d==0.18.0) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d==0.18.0) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d==0.18.0) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from Flask<3.1,>=1.0.4->dash>=2.6.0->open3d==0.18.0) (1.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from importlib-metadata->dash>=2.6.0->open3d==0.18.0) (3.17.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d==0.18.0) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d==0.18.0) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d==0.18.0) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d==0.18.0) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d==0.18.0) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d==0.18.0) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d==0.18.0) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d==0.18.0) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d==0.18.0) (0.4.6)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d==0.18.0) (23.1.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d==0.18.0) (6.1.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d==0.18.0) (2023.7.1)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d==0.18.0) (1.3.10)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d==0.18.0) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d==0.18.0) (0.10.6)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from plotly>=5.0.0->dash>=2.6.0->open3d==0.18.0) (8.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from plotly>=5.0.0->dash>=2.6.0->open3d==0.18.0) (23.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from jupyter-core->nbformat>=5.7.0->open3d==0.18.0) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from jupyter-core->nbformat>=5.7.0->open3d==0.18.0) (305.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from requests->dash>=2.6.0->open3d==0.18.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from requests->dash>=2.6.0->open3d==0.18.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from requests->dash>=2.6.0->open3d==0.18.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from requests->dash>=2.6.0->open3d==0.18.0) (2024.2.2)\n",
      "Requirement already satisfied: six>=1.7.0 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from retrying->dash>=2.6.0->open3d==0.18.0) (1.16.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d==0.18.0) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets>=8.0.4->open3d==0.18.0) (0.2.13)\n",
      "Requirement already satisfied: executing in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.0.4->open3d==0.18.0) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.0.4->open3d==0.18.0) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\andrew jeon\\anaconda3\\envs\\vlmaps\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets>=8.0.4->open3d==0.18.0) (0.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: pytorch-lightning 1.8.1 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "!pip install open3d==0.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "colored_voxel_grid = o3d.geometry.VoxelGrid()\n",
    "colored_voxel_grid.voxel_size = VOXEL_SIZE\n",
    "\n",
    "# for voxel, mask in zip(voxel_grid.values(), predicts):\n",
    "#     #print(new_pallete[mask])\n",
    "#     o3d_voxel = o3d.geometry.Voxel(voxel.coordinates, color_palette[mask])\n",
    "#     colored_voxel_grid.add_voxel(o3d_voxel)\n",
    "\n",
    "for coordinate, mask in zip(coordinates, predicts):\n",
    "    o3d_voxel = o3d.geometry.Voxel(coordinate, color_palette[mask])\n",
    "    colored_voxel_grid.add_voxel(o3d_voxel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o3d.visualization.draw_geometries([voxel_grid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Open3D WARNING] GLFW Error: WGL: Failed to make context current: The handle is invalid. \n",
      "[Open3D WARNING] GLFW Error: WGL: Failed to make context current: The requested transformation operation is not supported. \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window(visible=True)\n",
    "# Call only after creating visualizer window.\n",
    "vis.get_render_option().background_color = [0, 0.3, 0]\n",
    "vis.add_geometry(colored_voxel_grid)\n",
    "vis.run()\n",
    "vis.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "6db63e020538e79a80b9b328e157333fe21136baff2d1659d3fa8a4dfb8ecd33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
