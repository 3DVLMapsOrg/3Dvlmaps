{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0m1nbfGay6R"
   },
   "source": [
    "## Voxel VLMap Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "root_dir = !pwd\n",
    "root_dir = root_dir[0]\n",
    "data_dir = os.path.join(root_dir, \"data\", \"5LpN3gDmAk7_1\")\n",
    "\n",
    "colors_path = root_dir + \"/maps/colormap.npy\"\n",
    "features_path = root_dir + \"/maps/featuremap.npy\"\n",
    "coordinates_path = root_dir + \"/maps/coordinates.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bx_EfAUoay6S"
   },
   "outputs": [],
   "source": [
    "# setup parameters\n",
    "# @markdown meters per cell size\n",
    "cs = 0.05 # @param {type: \"number\"}\n",
    "voxel_size = 0.05 # @param {type: \"number\"}\n",
    "# @markdown map resolution (gs x gs)\n",
    "gs = 1000 # @param {type: \"integer\"}\n",
    "# @markdown camera height (used for filtering out points on the floor)\n",
    "camera_height = 1.5 # @param {type: \"number\"}\n",
    "# @markdown depth pixels subsample rate\n",
    "depth_sample_rate = 100 # @param {type: \"integer\"}\n",
    "# @markdown data where rgb, depth, pose are loaded and map are saved\n",
    "data_dir = data_dir # @param {type: \"string\"}\n",
    "\n",
    "img_save_dir = data_dir\n",
    "mask_version = 1\n",
    "crop_size = 480 # 480\n",
    "base_size = 520 # 520"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import open3d\n",
    "import clip\n",
    "# import torch\n",
    "lang = \"door,chair,ground,ceiling,other\"\n",
    "labels = lang.split(\",\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_version = \"ViT-B/32\"\n",
    "clip_feat_dim = {'RN50': 1024, 'RN101': 512, 'RN50x4': 640, 'RN50x16': 768,\n",
    "                'RN50x64': 1024, 'ViT-B/32': 512, 'ViT-B/16': 512, 'ViT-L/14': 768}[clip_version]\n",
    "\n",
    "print(\"Loading CLIP model...\")\n",
    "clip_model, preprocess = clip.load(clip_version, device=device)  # clip.available_models()\n",
    "\n",
    "clip_model.to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'examples'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# import open3d\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclip_mapping_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_pose, load_semantic, load_obj2cls_dict, save_map, cvt_obj_id_2_cls_id, depth2pc, transform_pc, get_sim_cam_mat, pos2grid_id, project_point\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlseg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlseg_net\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSegEncNet\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlseg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madditional_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resize_image, pad_image, crop_image\n",
      "File \u001b[0;32m~/Documents/UW/Classes/SP24/CSE_571_Robotics/Project/vlmaps/3Dvlmaps/utils/clip_mapping_utils.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mexamples\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcontext\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# from utils.utils import *\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# from socratic_navigation import navigate_with_pose\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_ai2thor_pose\u001b[39m(pose_filepath):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'examples'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "# import open3d\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.clip_mapping_utils import load_pose, load_semantic, load_obj2cls_dict, save_map, cvt_obj_id_2_cls_id, depth2pc, transform_pc, get_sim_cam_mat, pos2grid_id, project_point\n",
    "from lseg.modules.models.lseg_net import LSegEncNet\n",
    "from lseg.additional_utils.models import resize_image, pad_image, crop_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Checkpoints...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from lseg.modules.models.lseg_net import LSegEncNet\n",
    "\n",
    "lang_token = clip.tokenize(labels)\n",
    "lang_token = lang_token.to(device)\n",
    "with torch.no_grad():\n",
    "    text_feats = clip_model.encode_text(lang_token)\n",
    "    text_feats = text_feats / text_feats.norm(dim=-1, keepdim=True)\n",
    "text_feats = text_feats.cpu().numpy()\n",
    "model = LSegEncNet(lang, arch_option=0,\n",
    "                    block_depth=0,\n",
    "                    activation='lrelu',\n",
    "                    crop_size=crop_size)\n",
    "model_state_dict = model.state_dict()\n",
    "print(\"Loading Checkpoints...\")\n",
    "if device == \"cpu\":\n",
    "    pretrained_state_dict = torch.load(\"lseg/checkpoints/demo_e200.ckpt\", map_location=torch.device('cpu'))\n",
    "else:\n",
    "    pretrained_state_dict = torch.load(\"lseg/checkpoints/demo_e200.ckpt\")\n",
    "pretrained_state_dict = {k.lstrip('net.'): v for k, v in pretrained_state_dict['state_dict'].items()}\n",
    "model_state_dict.update(pretrained_state_dict)\n",
    "model.load_state_dict(pretrained_state_dict)\n",
    "\n",
    "model.eval()\n",
    "if device == \"cuda\":\n",
    "    model = model.cuda()\n",
    "\n",
    "norm_mean= [0.5, 0.5, 0.5]\n",
    "norm_std = [0.5, 0.5, 0.5]\n",
    "padding = [0.0] * 3\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This thing outputs a image segmentation mask as a np array\n",
    "def get_lseg_feat(model: LSegEncNet, image: np.array, labels, transform, crop_size=480, \\\n",
    "                 base_size=520, norm_mean=[0.5, 0.5, 0.5], norm_std=[0.5, 0.5, 0.5]):\n",
    "    vis_image = image.copy()\n",
    "    if device == \"cpu\":\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    else:\n",
    "        image = transform(image).unsqueeze(0).cuda()\n",
    "    img = image[0].permute(1,2,0)\n",
    "    img = img * 0.5 + 0.5\n",
    "    \n",
    "    batch, _, h, w = image.size() # batch is dimension 1, ignoring channel, y, x\n",
    "    stride_rate = 2.0/3.0\n",
    "    stride = int(crop_size * stride_rate)\n",
    "\n",
    "    long_size = base_size\n",
    "    if h > w:\n",
    "        height = long_size\n",
    "        width = int(1.0 * w * long_size / h + 0.5)\n",
    "        short_size = width\n",
    "    else:\n",
    "        width = long_size\n",
    "        height = int(1.0 * h * long_size / w + 0.5)\n",
    "        short_size = height\n",
    "\n",
    "\n",
    "    cur_img = resize_image(image, height, width, **{'mode': 'bilinear', 'align_corners': True})\n",
    "\n",
    "    if long_size <= crop_size:\n",
    "        pad_img = pad_image(cur_img, norm_mean,\n",
    "                            norm_std, crop_size)\n",
    "        print(pad_img.shape)\n",
    "        with torch.no_grad():\n",
    "            outputs, logits = model(pad_img, labels)\n",
    "        outputs = crop_image(outputs, 0, height, 0, width)\n",
    "    else:\n",
    "        if short_size < crop_size:\n",
    "            # pad if needed\n",
    "            pad_img = pad_image(cur_img, norm_mean,\n",
    "                                norm_std, crop_size)\n",
    "        else:\n",
    "            pad_img = cur_img\n",
    "        _,_,ph,pw = pad_img.shape #.size()\n",
    "        assert(ph >= height and pw >= width)\n",
    "        h_grids = int(math.ceil(1.0 * (ph-crop_size)/stride)) + 1\n",
    "        w_grids = int(math.ceil(1.0 * (pw-crop_size)/stride)) + 1\n",
    "\n",
    "        if device == \"cpu\":\n",
    "            with torch.cuda.device_of(image):\n",
    "                with torch.no_grad():\n",
    "                    outputs = image.new().resize_(batch, model.out_c,ph,pw).zero_()\n",
    "                    logits_outputs = image.new().resize_(batch, len(labels),ph,pw).zero_()\n",
    "                count_norm = image.new().resize_(batch,1,ph,pw).zero_()\n",
    "        else:\n",
    "            with torch.cuda.device_of(image):\n",
    "                with torch.no_grad():\n",
    "                    outputs = image.new().resize_(batch, model.out_c,ph,pw).zero_().cuda()\n",
    "                    logits_outputs = image.new().resize_(batch, len(labels),ph,pw).zero_().cuda()\n",
    "                count_norm = image.new().resize_(batch,1,ph,pw).zero_().cuda()\n",
    "        # grid evaluation\n",
    "        for idh in range(h_grids):\n",
    "            for idw in range(w_grids):\n",
    "                h0 = idh * stride\n",
    "                w0 = idw * stride\n",
    "                h1 = min(h0 + crop_size, ph)\n",
    "                w1 = min(w0 + crop_size, pw)\n",
    "                crop_img = crop_image(pad_img, h0, h1, w0, w1)\n",
    "                # pad if needed\n",
    "                pad_crop_img = pad_image(crop_img, norm_mean,\n",
    "                                            norm_std, crop_size)\n",
    "                with torch.no_grad():\n",
    "                    output, logits = model(pad_crop_img, labels)\n",
    "                cropped = crop_image(output, 0, h1-h0, 0, w1-w0)\n",
    "                cropped_logits = crop_image(logits, 0, h1-h0, 0, w1-w0)\n",
    "                outputs[:,:,h0:h1,w0:w1] += cropped\n",
    "                logits_outputs[:,:,h0:h1,w0:w1] += cropped_logits\n",
    "                count_norm[:,:,h0:h1,w0:w1] += 1\n",
    "        assert((count_norm==0).sum()==0)\n",
    "        outputs = outputs / count_norm\n",
    "        logits_outputs = logits_outputs / count_norm\n",
    "        outputs = outputs[:,:,:height,:width]\n",
    "        logits_outputs = logits_outputs[:,:,:height,:width]\n",
    "    outputs = outputs.cpu()\n",
    "    outputs = outputs.numpy() # B, D, H, W\n",
    "    predicts = [torch.max(logit, 0)[1].cpu().numpy() for logit in logits_outputs]\n",
    "    pred = predicts[0]\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def load_depth(depth_filepath):\n",
    "    with open(depth_filepath, 'rb') as f:\n",
    "        depth = np.load(f)\n",
    "    return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading scene /Users/anthony/Documents/UW/Classes/SP24/CSE_571_Robotics/Project/vlmaps/data/5LpN3gDmAk7_1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1159"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"loading scene {img_save_dir}\")\n",
    "rgb_dir = os.path.join(img_save_dir, \"rgb\")\n",
    "depth_dir = os.path.join(img_save_dir, \"depth\")\n",
    "pose_dir = os.path.join(img_save_dir, \"pose\")\n",
    "semantic_dir = os.path.join(img_save_dir, \"semantic\")\n",
    "\n",
    "rgb_list = sorted(os.listdir(rgb_dir), key=lambda x: int(\n",
    "    x.split(\"_\")[-1].split(\".\")[0]))\n",
    "depth_list = sorted(os.listdir(depth_dir), key=lambda x: int(\n",
    "    x.split(\"_\")[-1].split(\".\")[0]))\n",
    "pose_list = sorted(os.listdir(pose_dir), key=lambda x: int(\n",
    "    x.split(\"_\")[-1].split(\".\")[0]))\n",
    "pose_list = sorted(os.listdir(pose_dir), key=lambda x: int(\n",
    "    x.split(\"_\")[-1].split(\".\")[0]))\n",
    "semantic_list = sorted(os.listdir(semantic_dir), key=lambda x: int(\n",
    "    x.split(\"_\")[-1].split(\".\")[0]))\n",
    "\n",
    "rgb_list = [os.path.join(rgb_dir, x) for x in rgb_list]\n",
    "depth_list = [os.path.join(depth_dir, x) for x in depth_list]\n",
    "pose_list = [os.path.join(pose_dir, x) for x in pose_list]\n",
    "semantic_list = [os.path.join(semantic_dir, x) for x in semantic_list]\n",
    "\n",
    "tf_list = []\n",
    "data_iter = zip(rgb_list, depth_list, semantic_list, pose_list)\n",
    "len(rgb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.voxel import Voxel, GetVoxelCoor\n",
    "\n",
    "# lang = \"door,chair,ground,ceiling,other\"\n",
    "lang = \"ground,other\"\n",
    "labels = lang.split(\",\")\n",
    "\n",
    "voxel_grid = {}\n",
    "\n",
    "for frame_index, data_sample in enumerate(data_iter): \n",
    "    # print(frame_index)   \n",
    "    rgb_path, depth_path, semantic_path, pose_path = data_sample\n",
    "    \n",
    "    bgr = cv2.imread(rgb_path)\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # read pose\n",
    "    pos, rot = load_pose(pose_path)  # z backward, y upward, x to the right\n",
    "    rot_ro_cam = np.eye(3)\n",
    "    rot_ro_cam[1, 1] = -1\n",
    "    rot_ro_cam[2, 2] = -1\n",
    "    rot = rot @ rot_ro_cam\n",
    "    pos[1] += camera_height\n",
    "\n",
    "    pose = np.eye(4)\n",
    "    pose[:3, :3] = rot\n",
    "    pose[:3, 3] = pos.reshape(-1)\n",
    "\n",
    "    tf_list.append(pose)\n",
    "    if len(tf_list) == 1:\n",
    "        init_tf_inv = np.linalg.inv(tf_list[0]) \n",
    "\n",
    "    tf = init_tf_inv @ pose\n",
    "\n",
    "    depth = load_depth(depth_path)\n",
    "    \n",
    "    # transform all points to the global frame\n",
    "    pc, mask = depth2pc(depth)\n",
    "    shuffle_mask = np.arange(pc.shape[1]) \n",
    "    np.random.shuffle(shuffle_mask)\n",
    "    shuffle_mask = shuffle_mask[::depth_sample_rate]\n",
    "    mask = mask[shuffle_mask]\n",
    "    pc = pc[:, shuffle_mask]\n",
    "    pc = pc[:, mask]\n",
    "    pc_global = transform_pc(pc, tf)\n",
    "\n",
    "    pix_feats = get_lseg_feat(model, rgb, labels, transform, crop_size, base_size, norm_mean, norm_std)\n",
    "\n",
    "    rgb_cam_mat = get_sim_cam_mat(rgb.shape[0], rgb.shape[1])\n",
    "    feat_cam_mat = get_sim_cam_mat(pix_feats.shape[2], pix_feats.shape[3])\n",
    "\n",
    "    for pixel_index, (p, p_local) in enumerate(zip(pc_global.T, pc.T)):\n",
    "\n",
    "        single_global_point = (tf @ np.vstack([p_local.reshape(3,1), np.ones((1, 1))]) )[:3]\n",
    "\n",
    "        rgb_px, rgb_py, rgb_pz = project_point(rgb_cam_mat, p_local)\n",
    "        rgb_v = rgb[rgb_py, rgb_px, :].reshape(1,3)\n",
    "\n",
    "        voxel_coor = GetVoxelCoor(single_global_point, voxel_size)\n",
    "        voxel_key = str(voxel_coor[0,0]) + \",\" + str(voxel_coor[1,0]) + \",\" + str(voxel_coor[2,0])\n",
    "        if voxel_key not in voxel_grid:\n",
    "            voxel_grid[voxel_key] = Voxel(voxel_coor, pix_feats.shape[1]) #if not in voxel grid, then make it\n",
    "        voxel_grid[voxel_key].update_color((rgb_v/ 255.0))\n",
    "         \n",
    "        px, py, pz = project_point(feat_cam_mat, p_local)\n",
    "        if not (px < 0 or py < 0 or px >= pix_feats.shape[3] or py >= pix_feats.shape[2]):\n",
    "            feat = pix_feats[0, :, py, px] #these are for finding the corresponding features for that pixel in the feature matrix\n",
    "            voxel_grid[voxel_key].update_feature(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/anthony/Documents/UW/Classes/SP24/CSE_571_Robotics/Project/vlmaps/maps/colormap.npy is saved.\n",
      "/Users/anthony/Documents/UW/Classes/SP24/CSE_571_Robotics/Project/vlmaps/maps/featuremap.npy is saved.\n",
      "/Users/anthony/Documents/UW/Classes/SP24/CSE_571_Robotics/Project/vlmaps/maps/coordinates.npy is saved.\n"
     ]
    }
   ],
   "source": [
    "save_map(colors_path, np.array([i.colors for i in voxel_grid.values()]))\n",
    "save_map(features_path, np.array([i.features for i in voxel_grid.values()]))\n",
    "save_map(coordinates_path, np.array([i.coordinates for i in voxel_grid.values()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Map from object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(443299, 3, 1)\n",
      "(443299, 512)\n",
      "(443299, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "from utils.clip_mapping_utils import load_map\n",
    "from utils.clip_utils import get_text_feats\n",
    "from utils.mp3dcat import mp3dcat\n",
    "import numpy as np\n",
    "\n",
    "colors = np.array([i.colors for i in voxel_grid.values()])\n",
    "features = np.array([i.features for i in voxel_grid.values()])\n",
    "coordinates = np.array([i.coordinates for i in voxel_grid.values()])\n",
    "\n",
    "print(colors.shape)\n",
    "print(features.shape)\n",
    "print(coordinates.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Map from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([441861, 3, 1])\n",
      "torch.Size([441861, 512])\n",
      "torch.Size([441861, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "from utils.clip_mapping_utils import load_map\n",
    "from utils.clip_utils import get_text_feats\n",
    "from utils.mp3dcat import mp3dcat\n",
    "import numpy as np\n",
    "\n",
    "colors = torch.tensor(load_map(colors_path))\n",
    "features = torch.tensor(load_map(features_path), dtype=torch.float32)\n",
    "coordinates = torch.tensor(load_map(coordinates_path), dtype=torch.int)\n",
    "\n",
    "print(colors.shape)\n",
    "print(features.shape)\n",
    "print(coordinates.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 416/416 [02:30<00:00,  2.77it/s]\n"
     ]
    }
   ],
   "source": [
    "min_v, _ = torch.min(coordinates, dim=0)\n",
    "\n",
    "coordinates = coordinates - min_v\n",
    "\n",
    "max_v, _ = torch.max(coordinates, dim=0)\n",
    "max_v += 1\n",
    "\n",
    "grid = torch.zeros((max_v[0]+1, max_v[1]+1, max_v[2]+1, 1), dtype=torch.int32)\n",
    "\n",
    "for i, coor in enumerate(coordinates):\n",
    "    grid[coor[0], coor[1], coor[2]] = i\n",
    "\n",
    "kernel_size = 3\n",
    "\n",
    "new_coord = np.zeros((features.shape[0],3))\n",
    "new_feat = np.zeros((features.shape))\n",
    "\n",
    "index = 0\n",
    "kernel_bound = int(kernel_size/2)\n",
    "for x in tqdm(range(grid.shape[0]-kernel_bound)):\n",
    "    for y in range(grid.shape[1]-kernel_bound):\n",
    "        for z in range(grid.shape[2]-kernel_bound):\n",
    "            if(grid[x,y,z] == 0):\n",
    "                continue\n",
    "            lower_x = 0 if x == 0 else x - kernel_bound\n",
    "            lower_y = 0 if y == 0 else y - kernel_bound\n",
    "            lower_z = 0 if z == 0 else z - kernel_bound\n",
    "            voxel_patch = grid[lower_x:x+kernel_bound+1, lower_y:y+kernel_bound+1, lower_z:z+kernel_bound+1]\n",
    "            voxel_patch_feat = features[voxel_patch.flatten()[voxel_patch.flatten() != 0]]\n",
    "\n",
    "            new_feat[index] = torch.mean(voxel_patch_feat, dim=0)\n",
    "            new_coord[index] = [x,y,z]\n",
    "            index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"sofa, cealing, cushion, floor, other\"\n",
    "# lang = mp3dcat\n",
    "lang = lang.split(\",\")\n",
    "text_feats = get_text_feats(lang, clip_model, clip_feat_dim)\n",
    "\n",
    "scores_list = new_feat @ text_feats.T\n",
    "\n",
    "predicts = np.argmax(scores_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang = \"sofa, plant, towel, sink, food, other\"\n",
    "lang = \"sofa, cealing, cushion, floor, other\"\n",
    "lang = lang.split(\",\")\n",
    "text_feats = get_text_feats(lang, clip_model, clip_feat_dim)\n",
    "\n",
    "scores_list = features @ text_feats.T\n",
    "\n",
    "predicts = np.argmax(scores_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = mp3dcat # lang is all the matterport classes\n",
    "text_feats = get_text_feats(lang, clip_model, clip_feat_dim)\n",
    "\n",
    "scores_list = features @ text_feats.T\n",
    "\n",
    "predicts = np.argmax(scores_list, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lang' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcm\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m num_categories \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mlang\u001b[49m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Choose a colormap (e.g., inferno, plasma, viridis)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m cmap \u001b[38;5;241m=\u001b[39m cm\u001b[38;5;241m.\u001b[39mplasma\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lang' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_categories = len(lang)\n",
    "\n",
    "# Choose a colormap (e.g., inferno, plasma, viridis)\n",
    "cmap = cm.plasma\n",
    "\n",
    "# Generate an array of values from 0 to 1 representing the colormap index\n",
    "values = np.linspace(0, 1, num_categories)\n",
    "\n",
    "# Get RGB colors from the colormap for each value\n",
    "color_palette = cmap(values)[:,0:3]\n",
    "\n",
    "# round colors\n",
    "color_palette = np.round(color_palette, 2)\n",
    "\n",
    "plt.figure(figsize=(3,num_categories/3))\n",
    "bars = plt.barh(lang, [1 for i in range(num_categories)], color=color_palette)\n",
    "\n",
    "plt.gca().xaxis.set_visible(False)\n",
    "plt.margins(y=0.0)\n",
    "plt.box(False)\n",
    "plt.tick_params(left=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Segmented Voxel Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.open3d import create_voxel\n",
    "\n",
    "voxel_grid = create_voxel(new_coord, predicts, color_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.open3d import create_voxel\n",
    "\n",
    "voxel_grid = create_voxel(coordinates, predicts, color_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Real Voxel Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.open3d import create_real_rgb_voxel\n",
    "\n",
    "voxel_grid = create_real_rgb_voxel(coordinates, colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.open3d import create_white_voxel\n",
    "\n",
    "voxel_grid = create_white_voxel(coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Voxel Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[Open3D WARNING] GLFW Error: Cocoa: Failed to find service port for display\u001b[0;m\n"
     ]
    }
   ],
   "source": [
    "from utils.open3d import visualize_map\n",
    "\n",
    "visualize_map(voxel_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "6db63e020538e79a80b9b328e157333fe21136baff2d1659d3fa8a4dfb8ecd33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
