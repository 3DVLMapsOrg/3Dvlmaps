{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0m1nbfGay6R"
   },
   "source": [
    "## Voxel VLMap Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "data_dir = r\"C:\\Users\\Andrew Jeon\\OneDrive\\Desktop\\vlmaps\\data\\5LpN3gDmAk7_1\"\n",
    "root_dir = r\"C:\\Users\\Andrew Jeon\\OneDrive\\Desktop\\vlmaps\"\n",
    "colors_path = root_dir + \"/maps/colormap.npy\"\n",
    "features_path = root_dir + \"/maps/featuremap.npy\"\n",
    "coordinates_path = root_dir + \"/maps/coordinates.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "bx_EfAUoay6S"
   },
   "outputs": [],
   "source": [
    "# setup parameters\n",
    "# @markdown meters per cell size\n",
    "cs = 0.05 # @param {type: \"number\"}\n",
    "voxel_size = 0.05 # @param {type: \"number\"}\n",
    "# @markdown map resolution (gs x gs)\n",
    "gs = 1000 # @param {type: \"integer\"}\n",
    "# @markdown camera height (used for filtering out points on the floor)\n",
    "camera_height = 1.5 # @param {type: \"number\"}\n",
    "# @markdown depth pixels subsample rate\n",
    "depth_sample_rate = 100 # @param {type: \"integer\"}\n",
    "# @markdown data where rgb, depth, pose are loaded and map are saved\n",
    "data_dir = data_dir # @param {type: \"string\"}\n",
    "\n",
    "img_save_dir = data_dir\n",
    "mask_version = 1\n",
    "crop_size = 480 # 480\n",
    "base_size = 520 # 520"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import open3d\n",
    "import clip\n",
    "# import torch\n",
    "lang = \"door,chair,ground,ceiling,other\"\n",
    "labels = lang.split(\",\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_version = \"ViT-B/32\"\n",
    "clip_feat_dim = {'RN50': 1024, 'RN101': 512, 'RN50x4': 640, 'RN50x16': 768,\n",
    "                'RN50x64': 1024, 'ViT-B/32': 512, 'ViT-B/16': 512, 'ViT-L/14': 768}[clip_version]\n",
    "\n",
    "print(\"Loading CLIP model...\")\n",
    "clip_model, preprocess = clip.load(clip_version, device=device)  # clip.available_models()\n",
    "\n",
    "clip_model.to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "# import open3d\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.clip_mapping_utils import load_pose, load_semantic, load_obj2cls_dict, save_map, cvt_obj_id_2_cls_id, depth2pc, transform_pc, get_sim_cam_mat, pos2grid_id, project_point\n",
    "from lseg.modules.models.lseg_net import LSegEncNet\n",
    "from lseg.additional_utils.models import resize_image, pad_image, crop_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Checkpoints...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from lseg.modules.models.lseg_net import LSegEncNet\n",
    "\n",
    "lang_token = clip.tokenize(labels)\n",
    "lang_token = lang_token.to(device)\n",
    "with torch.no_grad():\n",
    "    text_feats = clip_model.encode_text(lang_token)\n",
    "    text_feats = text_feats / text_feats.norm(dim=-1, keepdim=True)\n",
    "text_feats = text_feats.cpu().numpy()\n",
    "model = LSegEncNet(lang, arch_option=0,\n",
    "                    block_depth=0,\n",
    "                    activation='lrelu',\n",
    "                    crop_size=crop_size)\n",
    "model_state_dict = model.state_dict()\n",
    "print(\"Loading Checkpoints...\")\n",
    "if device == \"cpu\":\n",
    "    pretrained_state_dict = torch.load(\"lseg/checkpoints/demo_e200.ckpt\", map_location=torch.device('cpu'))\n",
    "else:\n",
    "    pretrained_state_dict = torch.load(\"lseg/checkpoints/demo_e200.ckpt\")\n",
    "pretrained_state_dict = {k.lstrip('net.'): v for k, v in pretrained_state_dict['state_dict'].items()}\n",
    "model_state_dict.update(pretrained_state_dict)\n",
    "model.load_state_dict(pretrained_state_dict)\n",
    "\n",
    "model.eval()\n",
    "if device == \"cuda\":\n",
    "    model = model.cuda()\n",
    "\n",
    "norm_mean= [0.5, 0.5, 0.5]\n",
    "norm_std = [0.5, 0.5, 0.5]\n",
    "padding = [0.0] * 3\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This thing outputs a image segmentation mask as a np array\n",
    "def get_lseg_feat(model: LSegEncNet, image: np.array, labels, transform, crop_size=480, \\\n",
    "                 base_size=520, norm_mean=[0.5, 0.5, 0.5], norm_std=[0.5, 0.5, 0.5]):\n",
    "    vis_image = image.copy()\n",
    "    if device == \"cpu\":\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    else:\n",
    "        image = transform(image).unsqueeze(0).cuda()\n",
    "    img = image[0].permute(1,2,0)\n",
    "    img = img * 0.5 + 0.5\n",
    "    \n",
    "    batch, _, h, w = image.size() # batch is dimension 1, ignoring channel, y, x\n",
    "    stride_rate = 2.0/3.0\n",
    "    stride = int(crop_size * stride_rate)\n",
    "\n",
    "    long_size = base_size\n",
    "    if h > w:\n",
    "        height = long_size\n",
    "        width = int(1.0 * w * long_size / h + 0.5)\n",
    "        short_size = width\n",
    "    else:\n",
    "        width = long_size\n",
    "        height = int(1.0 * h * long_size / w + 0.5)\n",
    "        short_size = height\n",
    "\n",
    "\n",
    "    cur_img = resize_image(image, height, width, **{'mode': 'bilinear', 'align_corners': True})\n",
    "\n",
    "    if long_size <= crop_size:\n",
    "        pad_img = pad_image(cur_img, norm_mean,\n",
    "                            norm_std, crop_size)\n",
    "        print(pad_img.shape)\n",
    "        with torch.no_grad():\n",
    "            outputs, logits = model(pad_img, labels)\n",
    "        outputs = crop_image(outputs, 0, height, 0, width)\n",
    "    else:\n",
    "        if short_size < crop_size:\n",
    "            # pad if needed\n",
    "            pad_img = pad_image(cur_img, norm_mean,\n",
    "                                norm_std, crop_size)\n",
    "        else:\n",
    "            pad_img = cur_img\n",
    "        _,_,ph,pw = pad_img.shape #.size()\n",
    "        assert(ph >= height and pw >= width)\n",
    "        h_grids = int(math.ceil(1.0 * (ph-crop_size)/stride)) + 1\n",
    "        w_grids = int(math.ceil(1.0 * (pw-crop_size)/stride)) + 1\n",
    "\n",
    "        if device == \"cpu\":\n",
    "            with torch.cuda.device_of(image):\n",
    "                with torch.no_grad():\n",
    "                    outputs = image.new().resize_(batch, model.out_c,ph,pw).zero_()\n",
    "                    logits_outputs = image.new().resize_(batch, len(labels),ph,pw).zero_()\n",
    "                count_norm = image.new().resize_(batch,1,ph,pw).zero_()\n",
    "        else:\n",
    "            with torch.cuda.device_of(image):\n",
    "                with torch.no_grad():\n",
    "                    outputs = image.new().resize_(batch, model.out_c,ph,pw).zero_().cuda()\n",
    "                    logits_outputs = image.new().resize_(batch, len(labels),ph,pw).zero_().cuda()\n",
    "                count_norm = image.new().resize_(batch,1,ph,pw).zero_().cuda()\n",
    "        # grid evaluation\n",
    "        for idh in range(h_grids):\n",
    "            for idw in range(w_grids):\n",
    "                h0 = idh * stride\n",
    "                w0 = idw * stride\n",
    "                h1 = min(h0 + crop_size, ph)\n",
    "                w1 = min(w0 + crop_size, pw)\n",
    "                crop_img = crop_image(pad_img, h0, h1, w0, w1)\n",
    "                # pad if needed\n",
    "                pad_crop_img = pad_image(crop_img, norm_mean,\n",
    "                                            norm_std, crop_size)\n",
    "                with torch.no_grad():\n",
    "                    output, logits = model(pad_crop_img, labels)\n",
    "                cropped = crop_image(output, 0, h1-h0, 0, w1-w0)\n",
    "                cropped_logits = crop_image(logits, 0, h1-h0, 0, w1-w0)\n",
    "                outputs[:,:,h0:h1,w0:w1] += cropped\n",
    "                logits_outputs[:,:,h0:h1,w0:w1] += cropped_logits\n",
    "                count_norm[:,:,h0:h1,w0:w1] += 1\n",
    "        assert((count_norm==0).sum()==0)\n",
    "        outputs = outputs / count_norm\n",
    "        logits_outputs = logits_outputs / count_norm\n",
    "        outputs = outputs[:,:,:height,:width]\n",
    "        logits_outputs = logits_outputs[:,:,:height,:width]\n",
    "    outputs = outputs.cpu()\n",
    "    outputs = outputs.numpy() # B, D, H, W\n",
    "    predicts = [torch.max(logit, 0)[1].cpu().numpy() for logit in logits_outputs]\n",
    "    pred = predicts[0]\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def load_depth(depth_filepath):\n",
    "    with open(depth_filepath, 'rb') as f:\n",
    "        depth = np.load(f)\n",
    "    return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading scene C:\\Users\\Andrew Jeon\\OneDrive\\Desktop\\vlmaps\\data\\5LpN3gDmAk7_1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1159"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"loading scene {img_save_dir}\")\n",
    "rgb_dir = os.path.join(img_save_dir, \"rgb\")\n",
    "depth_dir = os.path.join(img_save_dir, \"depth\")\n",
    "pose_dir = os.path.join(img_save_dir, \"pose\")\n",
    "semantic_dir = os.path.join(img_save_dir, \"semantic\")\n",
    "\n",
    "rgb_list = sorted(os.listdir(rgb_dir), key=lambda x: int(\n",
    "    x.split(\"_\")[-1].split(\".\")[0]))\n",
    "depth_list = sorted(os.listdir(depth_dir), key=lambda x: int(\n",
    "    x.split(\"_\")[-1].split(\".\")[0]))\n",
    "pose_list = sorted(os.listdir(pose_dir), key=lambda x: int(\n",
    "    x.split(\"_\")[-1].split(\".\")[0]))\n",
    "# pose_list = sorted(os.listdir(pose_dir), key=lambda x: int(\n",
    "#     x.split(\"_\")[-1].split(\".\")[0]))\n",
    "semantic_list = sorted(os.listdir(semantic_dir), key=lambda x: int(\n",
    "    x.split(\"_\")[-1].split(\".\")[0]))\n",
    "\n",
    "rgb_list = [os.path.join(rgb_dir, x) for x in rgb_list]\n",
    "depth_list = [os.path.join(depth_dir, x) for x in depth_list]\n",
    "pose_list = [os.path.join(pose_dir, x) for x in pose_list]\n",
    "semantic_list = [os.path.join(semantic_dir, x) for x in semantic_list]\n",
    "\n",
    "tf_list = []\n",
    "data_iter = zip(rgb_list, depth_list, semantic_list, pose_list)\n",
    "len(rgb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos, rot = load_pose(pose_list[1])\n",
    "rot_ro_cam = np.eye(3)\n",
    "rot_ro_cam[1, 1] = -1\n",
    "rot_ro_cam[2, 2] = -1\n",
    "rot = rot @ rot_ro_cam\n",
    "pos[1] += camera_height\n",
    "\n",
    "pose = np.eye(4)\n",
    "pose[:3, :3] = rot\n",
    "pose[:3, 3] = pos.reshape(-1)\n",
    "\n",
    "tf_list.append(pose)\n",
    "if len(tf_list) == 1:\n",
    "    init_tf_inv = np.linalg.inv(tf_list[0]) \n",
    "\n",
    "tf = init_tf_inv @ pose\n",
    "p = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = load_depth(depth_list[0])\n",
    "pc, mask = depth2pc(depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.voxel import Voxel, GetVoxelCoor\n",
    "\n",
    "# lang = \"door,chair,ground,ceiling,other\"\n",
    "lang = \"ground,other\"\n",
    "labels = lang.split(\",\")\n",
    "\n",
    "voxel_grid = {}\n",
    "\n",
    "for frame_index, data_sample in enumerate(data_iter): \n",
    "    # print(frame_index)   \n",
    "    rgb_path, depth_path, semantic_path, pose_path = data_sample\n",
    "    \n",
    "    bgr = cv2.imread(rgb_path)\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # read pose\n",
    "    pos, rot = load_pose(pose_path)  # z backward, y upward, x to the right\n",
    "    rot_ro_cam = np.eye(3)\n",
    "    rot_ro_cam[1, 1] = -1\n",
    "    rot_ro_cam[2, 2] = -1\n",
    "    rot = rot @ rot_ro_cam\n",
    "    pos[1] += camera_height\n",
    "\n",
    "    pose = np.eye(4)\n",
    "    pose[:3, :3] = rot\n",
    "    pose[:3, 3] = pos.reshape(-1)\n",
    "\n",
    "    tf_list.append(pose)\n",
    "    if len(tf_list) == 1:\n",
    "        init_tf_inv = np.linalg.inv(tf_list[0]) \n",
    "\n",
    "    tf = init_tf_inv @ pose\n",
    "\n",
    "    depth = load_depth(depth_path)\n",
    "    \n",
    "    # transform all points to the global frame\n",
    "    pc, mask = depth2pc(depth)\n",
    "    shuffle_mask = np.arange(pc.shape[1]) \n",
    "    np.random.shuffle(shuffle_mask)\n",
    "    shuffle_mask = shuffle_mask[::depth_sample_rate]\n",
    "    mask = mask[shuffle_mask]\n",
    "    pc = pc[:, shuffle_mask]\n",
    "    pc = pc[:, mask]\n",
    "    pc_global = transform_pc(pc, tf)\n",
    "\n",
    "    pix_feats = get_lseg_feat(model, rgb, labels, transform, crop_size, base_size, norm_mean, norm_std)\n",
    "\n",
    "    rgb_cam_mat = get_sim_cam_mat(rgb.shape[0], rgb.shape[1])\n",
    "    feat_cam_mat = get_sim_cam_mat(pix_feats.shape[2], pix_feats.shape[3])\n",
    "\n",
    "    for pixel_index, (p, p_local) in enumerate(zip(pc_global.T, pc.T)):\n",
    "\n",
    "        single_global_point = (tf @ np.vstack([p_local.reshape(3,1), np.ones((1, 1))]) )[:3]\n",
    "\n",
    "        rgb_px, rgb_py, rgb_pz = project_point(rgb_cam_mat, p_local)\n",
    "        rgb_v = rgb[rgb_py, rgb_px, :].reshape(1,3)\n",
    "\n",
    "        voxel_coor = GetVoxelCoor(single_global_point, voxel_size)\n",
    "        voxel_key = str(voxel_coor[0,0]) + \",\" + str(voxel_coor[1,0]) + \",\" + str(voxel_coor[2,0])\n",
    "        if voxel_key not in voxel_grid:\n",
    "            voxel_grid[voxel_key] = Voxel(voxel_coor, pix_feats.shape[1]) #if not in voxel grid, then make it\n",
    "        voxel_grid[voxel_key].update_color((rgb_v/ 255.0))\n",
    "         \n",
    "        px, py, pz = project_point(feat_cam_mat, p_local)\n",
    "        if not (px < 0 or py < 0 or px >= pix_feats.shape[3] or py >= pix_feats.shape[2]):\n",
    "            feat = pix_feats[0, :, py, px] #these are for finding the corresponding features for that pixel in the feature matrix\n",
    "            voxel_grid[voxel_key].update_feature(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'open3d.cpu.pybind.geometry.VoxelGrid' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m save_map(colors_path, np\u001b[38;5;241m.\u001b[39marray([i\u001b[38;5;241m.\u001b[39mcolors \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvoxel_grid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m()]))\n\u001b[0;32m      2\u001b[0m save_map(features_path, np\u001b[38;5;241m.\u001b[39marray([i\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m voxel_grid\u001b[38;5;241m.\u001b[39mvalues()]))\n\u001b[0;32m      3\u001b[0m save_map(coordinates_path, np\u001b[38;5;241m.\u001b[39marray([i\u001b[38;5;241m.\u001b[39mcoordinates \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m voxel_grid\u001b[38;5;241m.\u001b[39mvalues()]))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'open3d.cpu.pybind.geometry.VoxelGrid' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "save_map(colors_path, np.array([i.colors for i in voxel_grid.values()]))\n",
    "save_map(features_path, np.array([i.features for i in voxel_grid.values()]))\n",
    "save_map(coordinates_path, np.array([i.coordinates for i in voxel_grid.values()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Map from object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'open3d.cpu.pybind.geometry.VoxelGrid' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmp3dcat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mp3dcat\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m colors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([i\u001b[38;5;241m.\u001b[39mcolors \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mvoxel_grid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m()])\n\u001b[0;32m      7\u001b[0m features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([i\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m voxel_grid\u001b[38;5;241m.\u001b[39mvalues()])\n\u001b[0;32m      8\u001b[0m coordinates \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([i\u001b[38;5;241m.\u001b[39mcoordinates \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m voxel_grid\u001b[38;5;241m.\u001b[39mvalues()])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'open3d.cpu.pybind.geometry.VoxelGrid' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "from utils.clip_mapping_utils import load_map\n",
    "from utils.clip_utils import get_text_feats\n",
    "from utils.mp3dcat import mp3dcat\n",
    "import numpy as np\n",
    "\n",
    "colors = np.array([i.colors for i in voxel_grid.values()])\n",
    "features = np.array([i.features for i in voxel_grid.values()])\n",
    "coordinates = np.array([i.coordinates for i in voxel_grid.values()])\n",
    "\n",
    "print(colors.shape)\n",
    "print(features.shape)\n",
    "print(coordinates.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Map from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([456992, 1, 3])\n",
      "torch.Size([456992, 512])\n",
      "torch.Size([456992, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "from utils.clip_mapping_utils import load_map\n",
    "from utils.clip_utils import get_text_feats\n",
    "from utils.mp3dcat import mp3dcat\n",
    "import numpy as np\n",
    "\n",
    "colors = torch.tensor(load_map(colors_path))\n",
    "features = torch.tensor(load_map(features_path), dtype=torch.float32)\n",
    "coordinates = torch.tensor(load_map(coordinates_path), dtype=torch.int)\n",
    "\n",
    "print(colors.shape)\n",
    "print(features.shape)\n",
    "print(coordinates.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 399/399 [05:02<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "min_v, _ = torch.min(coordinates, dim=0)\n",
    "\n",
    "coordinates = coordinates - min_v\n",
    "\n",
    "max_v, _ = torch.max(coordinates, dim=0)\n",
    "max_v += 1\n",
    "\n",
    "grid = torch.zeros((max_v[0]+1, max_v[1]+1, max_v[2]+1, 1), dtype=torch.int32)\n",
    "\n",
    "for i, coor in enumerate(coordinates):\n",
    "    grid[coor[0], coor[1], coor[2]] = i\n",
    "\n",
    "kernel_size = 3\n",
    "\n",
    "new_coord = np.zeros((features.shape[0],3))\n",
    "new_feat = np.zeros((features.shape))\n",
    "\n",
    "index = 0\n",
    "kernel_bound = int(kernel_size/2)\n",
    "for x in tqdm(range(grid.shape[0]-kernel_bound)):\n",
    "    for y in range(grid.shape[1]-kernel_bound):\n",
    "        for z in range(grid.shape[2]-kernel_bound):\n",
    "            if(grid[x,y,z] == 0):\n",
    "                continue\n",
    "            lower_x = 0 if x == 0 else x - kernel_bound\n",
    "            lower_y = 0 if y == 0 else y - kernel_bound\n",
    "            lower_z = 0 if z == 0 else z - kernel_bound\n",
    "            voxel_patch = grid[lower_x:x+kernel_bound+1, lower_y:y+kernel_bound+1, lower_z:z+kernel_bound+1]\n",
    "            voxel_patch_feat = features[voxel_patch.flatten()[voxel_patch.flatten() != 0]]\n",
    "\n",
    "            new_feat[index] = torch.mean(voxel_patch_feat, dim=0)\n",
    "            new_coord[index] = [x,y,z]\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"wall, floor, chair, door, table, picture, cabinet, cushion, window, sofa, bed, curtain, plant, sink, stairs, ceiling\"\n",
    "# lang = mp3dcat\n",
    "lang = lang.split(\",\")\n",
    "text_feats = get_text_feats(lang, clip_model, clip_feat_dim)\n",
    "\n",
    "scores_list = new_feat @ text_feats.T\n",
    "\n",
    "predicts = np.argmax(scores_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang = \"sofa, plant, towel, sink, food, other\"\n",
    "lang = \"sofa, ceiling, cushion, floor, other\"\n",
    "lang = lang.split(\",\")\n",
    "text_feats = get_text_feats(lang, clip_model, clip_feat_dim)\n",
    "\n",
    "scores_list = features @ text_feats.T\n",
    "\n",
    "predicts = np.argmax(scores_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = mp3dcat # lang is all the matterport classes\n",
    "text_feats = get_text_feats(lang, clip_model, clip_feat_dim)\n",
    "\n",
    "scores_list = features @ text_feats.T\n",
    "\n",
    "predicts = np.argmax(scores_list, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x533.333 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_categories = len(lang)\n",
    "\n",
    "# Choose a colormap (e.g., inferno, plasma, viridis)\n",
    "cmap = cm.plasma\n",
    "\n",
    "# Generate an array of values from 0 to 1 representing the colormap index\n",
    "values = np.linspace(0, 1, num_categories)\n",
    "\n",
    "# Get RGB colors from the colormap for each value\n",
    "color_palette = cmap(values)[:,0:3]\n",
    "\n",
    "# round colors\n",
    "color_palette = np.round(color_palette, 2)\n",
    "\n",
    "plt.figure(figsize=(3,num_categories/3))\n",
    "bars = plt.barh(lang, [1 for i in range(num_categories)], color=color_palette)\n",
    "\n",
    "plt.gca().xaxis.set_visible(False)\n",
    "plt.margins(y=0.0)\n",
    "plt.box(False)\n",
    "plt.tick_params(left=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Segmented Voxel Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.open3d import create_voxel\n",
    "\n",
    "voxel_grid = create_voxel(coordinates, predicts, color_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.open3d import create_voxel\n",
    "\n",
    "voxel_grid = create_voxel(new_coord, predicts, color_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Real Voxel Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.open3d import create_real_rgb_voxel\n",
    "\n",
    "voxel_grid = create_real_rgb_voxel(coordinates, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Voxel Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running visualization\n"
     ]
    }
   ],
   "source": [
    "from utils.open3d import visualize_map\n",
    "\n",
    "visualize_map(voxel_grid)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "vlmaps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
